{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3hlxRYaLkMVswFZSNiiLO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Theory"],"metadata":{"id":"HT1adTIW_c2a"}},{"cell_type":"markdown","source":["1.  What is Logistic Regression, and how does it differ from Linear Regression?\n","- **Logistic Regression** and **Linear Regression** are both fundamental algorithms in statistical modeling and machine learning, but they serve different purposes and make different assumptions. Here’s a breakdown of each and their key distinctions:\n","\n","---\n","\n","## 1. Problem Type\n","\n","* **Linear Regression**\n","\n","  * **Task**: Predict a *continuous* outcome (e.g., house price, temperature).\n","  * **Example**: Estimating a student’s exam score based on hours studied.\n","\n","* **Logistic Regression**\n","\n","  * **Task**: Predict a *categorical* outcome, typically binary (e.g., spam vs. not spam).\n","  * **Example**: Classifying emails as “spam” (1) or “not spam” (0).\n","\n","---\n","\n","## 2. Model Formulation\n","\n","1. **Linear Regression**\n","\n","   * **Model**:\n","\n","     $$\n","     \\hat y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n","     $$\n","   * **Interpretation**: The prediction $\\hat y$ is a weighted sum of the input features.\n","\n","2. **Logistic Regression**\n","\n","   * **Model**:\n","\n","     $$\n","     z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n","     \\quad,\\quad\n","     \\hat p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n","     $$\n","   * **Interpretation**:\n","\n","     * First compute the linear combination $z$.\n","     * Then apply the **sigmoid (logistic) function** $\\sigma(z)$ to squash $z$ into the range $(0,1)$.\n","     * $\\hat p$ is interpreted as the probability of the “positive” class (e.g., class = 1).\n","\n","---\n","\n","## 3. Loss (Cost) Functions\n","\n","* **Linear Regression**\n","\n","  * **Mean Squared Error (MSE)**:\n","\n","    $$\n","    J(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat y_i)^2\n","    $$\n","  * **Optimization**: Often solved in closed form (normal equations) or via gradient descent.\n","\n","* **Logistic Regression**\n","\n","  * **Log-Loss** (aka **Binary Cross-Entropy**):\n","\n","    $$\n","    J(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n\\Bigl[y_i\\log(\\hat p_i) + (1-y_i)\\log(1-\\hat p_i)\\Bigr]\n","    $$\n","  * **Optimization**: No closed-form; typically uses iterative methods like gradient descent or Newton’s method.\n","\n","---\n","\n","## 4. Output & Interpretation\n","\n","| Aspect             | Linear Regression        | Logistic Regression                                   |\n","| ------------------ | ------------------------ | ----------------------------------------------------- |\n","| **Output**         | Real number $\\hat y$     | Probability $\\hat p\\in(0,1)$                          |\n","| **Decision Rule**  | —                        | $\\hat y = 1$ if $\\hat p \\ge 0.5$ (threshold can vary) |\n","| **Interpretation** | Direct prediction of $y$ | Odds ratio: $\\log\\frac{p}{1-p}$ is linear in $x$      |\n","\n","---\n","\n","## 5. Assumptions & Considerations\n","\n","1. **Linearity**\n","\n","   * Both assume a linear relationship in the *log-odds* (for logistic) or in the *outcome* (for linear).\n","2. **Residuals**\n","\n","   * Linear regression residuals are assumed to be normally distributed with constant variance (homoscedasticity).\n","   * Logistic regression does not assume normal residuals—its probabilistic model handles variance inherently.\n","3. **Outliers & Leverage**\n","\n","   * Outliers can heavily influence linear regression; logistic regression is generally more robust in classification contexts but can still be affected by extreme feature values.\n","\n","---\n","\n","## 6. Use Cases & Extensions\n","\n","* **Linear Regression**\n","\n","  * Predicting prices, forecasting sales, modeling physical phenomena.\n","  * Extensions: *Ridge*, *Lasso*, *Polynomial Regression*, *Generalized Additive Models*.\n","\n","* **Logistic Regression**\n","\n","  * Medical diagnosis (disease vs. healthy), credit scoring (default vs. no default), email spam detection.\n","  * Extensions: *Multinomial Logistic Regression* (for >2 classes), *Regularized Logistic Regression* (L1/L2 penalties).\n","\n","---\n","\n","### Summary\n","\n","| Feature          | Linear Regression       | Logistic Regression                          |\n","| ---------------- | ----------------------- | -------------------------------------------- |\n","| **Goal**         | Estimate continuous $y$ | Estimate probability of class membership     |\n","| **Model Output** | $\\hat y\\in\\mathbb{R}$   | $\\hat p\\in(0,1)$ via sigmoid of linear model |\n","| **Loss**         | Mean Squared Error      | Log-Loss (Cross-Entropy)                     |\n","| **Decision**     | —                       | Threshold probability to classify            |\n","\n","In essence, **linear regression** is your go-to when the target is continuous, whereas **logistic regression** adapts that linear idea to the classification realm by wrapping it in a sigmoid function and optimizing a likelihood-based loss.\n"],"metadata":{"id":"7mtdW-mH_hz2"}},{"cell_type":"markdown","source":["2.  What is the mathematical equation of Logistic Regression?\n","- The core of logistic regression can be written in two equivalent ways:\n","\n","---\n","\n","1. **Probability (sigmoid) form**\n","\n","   $$\n","   \\hat p = P(y = 1 \\mid \\mathbf{x}) \\;=\\; \\sigma\\!\\bigl(z\\bigr)\n","   \\quad\\text{where}\\quad\n","   z = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n","   $$\n","\n","   and the **sigmoid** (logistic) function is\n","\n","   $$\n","   \\sigma(z) = \\frac{1}{1 + e^{-\\,z}}.\n","   $$\n","\n","   So explicitly:\n","\n","   $$\n","   P(y=1 \\mid \\mathbf{x})\n","   = \\frac{1}{1 + \\exp\\bigl[-(\\beta_0 + \\sum_{j=1}^p \\beta_j x_j)\\bigr]}.\n","   $$\n","\n","2. **Log-odds (logit) form**\n","   Taking the log of the odds ratio gives a *linear* expression in the inputs:\n","\n","   $$\n","   \\log\\!\\Bigl(\\frac{P(y=1 \\mid \\mathbf{x})}{P(y=0 \\mid \\mathbf{x})}\\Bigr)\n","   = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p.\n","   $$\n","\n","---\n","\n","* **Interpretation**\n","\n","  * $\\beta_0$ is the intercept (baseline log-odds when all $x_j=0$).\n","  * Each $\\beta_j$ is the change in the log-odds of $y=1$ per one-unit increase in $x_j$.\n","\n","* **From logit to probability**\n","\n","  $$\n","  P(y=1 \\mid \\mathbf{x})\n","  = \\frac{e^{\\beta_0 + \\sum_j \\beta_j x_j}}\n","         {1 + e^{\\beta_0 + \\sum_j \\beta_j x_j}}.\n","  $$\n","\n","That pair of equations—sigmoid for probability, logit for linearity—constitutes the mathematical heart of logistic regression.\n"],"metadata":{"id":"tIgfQwo1_7_j"}},{"cell_type":"markdown","source":["3.  Why do we use the Sigmoid function in Logistic Regression?\n","- We use the **sigmoid (logistic) function** in logistic regression for several interrelated reasons:\n","\n","1. **Squashes Into a Probability**\n","\n","   * The raw linear combination\n","\n","     $$\n","       z = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n","     $$\n","\n","     can take any real value $(-\\infty, +\\infty)$.\n","   * The sigmoid\n","\n","     $$\n","       \\sigma(z) = \\frac{1}{1 + e^{-z}}\n","     $$\n","\n","     maps $z$ smoothly into the interval $(0,1)$, which we interpret as\n","\n","     $$\n","       \\hat p = P(y=1 \\mid x) \\in (0,1).\n","     $$\n","\n","2. **Monotonic & Smooth**\n","\n","   * $\\sigma(z)$ is strictly increasing: larger $z$ → larger $\\sigma(z)$.\n","   * It’s infinitely differentiable, which makes gradient-based optimization (like gradient descent or Newton’s method) straightforward.\n","\n","3. **Link to Log-Odds (Linear Relationship)**\n","\n","   * The inverse of the sigmoid is the **logit**:\n","\n","     $$\n","       \\operatorname{logit}(p) = \\ln\\frac{p}{1-p}\\quad\\Longrightarrow\\quad z = \\beta_0 + \\sum_j\\beta_j x_j.\n","     $$\n","   * This gives a direct linear relationship between the predictors and the log-odds of the outcome, which is intuitive and interpretable:\n","     $\\beta_j$ is the change in log-odds per unit change in $x_j$.\n","\n","4. **Convenient Derivative**\n","\n","   * $\\sigma'(z) = \\sigma(z)\\bigl[1 - \\sigma(z)\\bigr]$.\n","   * This simple form lets us write the gradient of the log-likelihood cleanly, speeding up convergence in training.\n","\n","5. **Probabilistic Foundation**\n","\n","   * Under the hood, logistic regression can be derived as a special case of a **generalized linear model** (GLM) with a binomial distribution and the logit link.\n","   * The sigmoid arises naturally when you maximize the binomial likelihood with that link function.\n","\n","---\n","\n","**In summary**, the sigmoid function is the perfect bridge between a linear predictor and a probability: it ensures outputs lie in $(0,1)$, preserves monotonicity for interpretability, has a mathematically tractable derivative, and aligns with the statistical foundations of binary classification.\n"],"metadata":{"id":"JMRe0ZqRAOo-"}},{"cell_type":"markdown","source":["4.  What is the cost function of Logistic Regression?\n","- The cost function (also called the loss function) that logistic regression minimizes is the **binary cross-entropy** (or **log-loss**). It measures how “surprised” the model is by the true labels given its predicted probabilities. Formally:\n","\n","---\n","\n","### 1. Per-example loss\n","\n","For a single training example $(\\mathbf{x}^{(i)}, y^{(i)})$, with model output\n","\n","$$\n","\\hat p^{(i)} = h_\\beta(\\mathbf{x}^{(i)})\n","= \\sigma\\bigl(\\beta_0 + \\sum_{j=1}^p \\beta_j\\,x_j^{(i)}\\bigr),\n","$$\n","\n","the loss is\n","\n","$$\n","\\ell\\bigl(\\hat p^{(i)},\\,y^{(i)}\\bigr)\n","= -\\,\\Bigl[y^{(i)}\\;\\log\\!\\bigl(\\hat p^{(i)}\\bigr)\n","         \\;+\\;(1 - y^{(i)})\\;\\log\\!\\bigl(1 - \\hat p^{(i)}\\bigr)\\Bigr].\n","$$\n","\n","* If $y=1$, the loss reduces to $-\\log(\\hat p)$: you’re penalized heavily if the model assigns low probability to the true class.\n","* If $y=0$, the loss is $-\\log(1-\\hat p)$: you’re penalized if the model gives too high a probability to the positive class.\n","\n","---\n","\n","### 2. Overall cost\n","\n","Averaging over all $m$ training examples gives the cost function $J(\\beta)$:\n","\n","$$\n","J(\\beta)\n","= \\frac{1}{m}\\sum_{i=1}^m \\ell\\bigl(\\hat p^{(i)},\\,y^{(i)}\\bigr)\n","= -\\,\\frac{1}{m}\\sum_{i=1}^m\n","   \\Bigl[y^{(i)}\\log\\!\\bigl(h_\\beta(\\mathbf{x}^{(i)})\\bigr)\n","         + (1 - y^{(i)})\\log\\!\\bigl(1 - h_\\beta(\\mathbf{x}^{(i)})\\bigr)\\Bigr].\n","$$\n","\n","Minimizing $J(\\beta)$ via gradient descent (or a variant) yields the maximum-likelihood estimates of the parameters $\\beta$.\n","\n","---\n","\n","### 3. Regularized version (optional)\n","\n","To prevent overfitting, one often adds an $L_2$ penalty on the weights (excluding the intercept):\n","\n","$$\n","J_{\\!\\lambda}(\\beta)\n","= J(\\beta) \\;+\\; \\frac{\\lambda}{2m}\\sum_{j=1}^p \\beta_j^2.\n","$$\n","\n","---\n","\n","**Key takeaway**: logistic regression’s cost function is the average of the negative log-likelihoods under a Bernoulli model, commonly known as binary cross-entropy or log-loss.\n","\n","---"],"metadata":{"id":"iQjVRCpYAcCT"}},{"cell_type":"markdown","source":["5. What is Regularization in Logistic Regression? Why is it needed?\n","- **Regularization** in logistic regression refers to adding a penalty term to the cost function that discourages overly large (or complex) model parameters. It’s a way of controlling model complexity to improve generalization on unseen data.\n","\n","---\n","\n","## 1. Why Regularize?\n","\n","1. **Prevent Overfitting**\n","\n","   * Without regularization, the model can drive some coefficients $\\beta_j$ to very large values to perfectly fit noise or outliers in the training set.\n","   * Large weights often translate to high‐variance models that perform poorly on new data.\n","\n","2. **Handle Multicollinearity**\n","\n","   * When features are highly correlated, the ordinary maximum‐likelihood solution can become numerically unstable (coefficients blow up).\n","   * Regularization adds stability by shrinking correlated coefficients toward each other.\n","\n","3. **Feature Selection (L₁ Regularization)**\n","\n","   * L₁ (lasso) regularization can drive some $\\beta_j$ exactly to zero, effectively selecting a simpler subset of features.\n","\n","---\n","\n","## 2. How It’s Incorporated\n","\n","Starting with the ordinary (unregularized) cost:\n","\n","$$\n","J(\\beta)\n","= -\\frac{1}{m}\\sum_{i=1}^m\n","   \\Bigl[y^{(i)}\\log h_\\beta(\\mathbf{x}^{(i)})\n","         + (1 - y^{(i)})\\log\\bigl(1 - h_\\beta(\\mathbf{x}^{(i)})\\bigr)\\Bigr]\n","$$\n","\n","we add a penalty term $R(\\beta)$ scaled by a hyperparameter $\\lambda$:\n","\n","$$\n","J_{\\!\\lambda}(\\beta)\n","= J(\\beta) \\;+\\; \\frac{\\lambda}{m}\\,R(\\beta).\n","$$\n","\n","Common choices for $R(\\beta)$ are:\n","\n","1. **L₂ Regularization (Ridge)**\n","\n","   $$\n","   R(\\beta) = \\frac{1}{2}\\sum_{j=1}^p \\beta_j^2\n","   \\quad\\Longrightarrow\\quad\n","   J_{\\!\\lambda}(\\beta)\n","   = J(\\beta) + \\frac{\\lambda}{2m}\\sum_{j=1}^p \\beta_j^2.\n","   $$\n","\n","   * Shrinks all coefficients smoothly toward zero.\n","   * Tends to keep most features but with smaller weights.\n","\n","2. **L₁ Regularization (Lasso)**\n","\n","   $$\n","   R(\\beta) = \\sum_{j=1}^p \\lvert\\beta_j\\rvert\n","   \\quad\\Longrightarrow\\quad\n","   J_{\\!\\lambda}(\\beta)\n","   = J(\\beta) + \\frac{\\lambda}{m}\\sum_{j=1}^p \\lvert\\beta_j\\rvert.\n","   $$\n","\n","   * Can drive some coefficients exactly to zero → feature selection.\n","   * Often leads to sparse solutions.\n","\n","3. **Elastic Net**\n","   A weighted combination of L₁ and L₂:\n","\n","   $$\n","   R(\\beta)\n","   = \\alpha\\sum_{j} \\lvert\\beta_j\\rvert\n","     + \\frac{1-\\alpha}{2}\\sum_{j}\\beta_j^2.\n","   $$\n","\n","   * Balances sparsity (L₁) with coefficient stability (L₂).\n","\n","---\n","\n","## 3. The Role of the Regularization Parameter $\\lambda$\n","\n","* **$\\lambda = 0$** → no regularization (standard logistic regression).\n","* **Large $\\lambda$** → heavy penalty → all $\\beta_j$ shrink toward zero → simpler model.\n","* **Selecting $\\lambda$** is typically done via cross‐validation.\n","\n","---\n","\n","## 4. Practical Benefits\n","\n","* **Improved Generalization**: By penalizing complexity, the model is less likely to overfit and will often perform better on hold‐out or test data.\n","* **Numerical Stability**: Prevents coefficient explosion when features are correlated.\n","* **Model Interpretability**: Particularly with L₁, yields a sparse model highlighting the most important predictors.\n","\n","---\n","\n","### In a Nutshell\n","\n","Regularization adds a controlled penalty on the size of coefficients in logistic regression. This balances the trade‐off between fitting the training data well and keeping the model simple enough to generalize reliably.\n","\n","---"],"metadata":{"id":"C1wICAiOAwvl"}},{"cell_type":"markdown","source":["6.  Explain the difference between Lasso, Ridge, and Elastic Net regression.\n","- **Lasso**, **Ridge**, and **Elastic Net** are three popular regularization techniques that modify the ordinary least-squares (or likelihood) objective by adding a penalty on the size of the coefficients. This helps control overfitting, improve numerical stability, and, in some cases, perform feature selection. Here’s how they differ:\n","\n","---\n","\n","## 1. Penalty Terms\n","\n","Let $\\mathbf{β} = (\\beta_1, \\dots, \\beta_p)$ be the vector of model coefficients (excluding any intercept), and let\n","\n","$$\n","J(\\mathbf{β}) = \\frac{1}{n}\\sum_{i=1}^n \\bigl(y_i - \\hat y_i\\bigr)^2\n","$$\n","\n","be the ordinary (unregularized) mean-squared error. We add a penalty $R(\\mathbf{β})$ weighted by a regularization parameter $\\lambda \\ge 0$:\n","\n","$$\n","J_{\\lambda}(\\mathbf{β}) = J(\\mathbf{β}) + \\lambda\\,R(\\mathbf{β}).\n","$$\n","\n","| **Method**      | **Penalty $R(\\mathbf{β})$**                      | **Effect**                                                                                    |                                 |                                                                           |\n","| --------------- | ------------------------------------------------ | --------------------------------------------------------------------------------------------- | ------------------------------- | ------------------------------------------------------------------------- |\n","| **Ridge**       | $\\displaystyle \\sum_{j=1}^p \\beta_j^2$ (L₂ norm) | Shrinks coefficients continuously toward zero; retains all features but with smaller weights. |                                 |                                                                           |\n","| **Lasso**       | (\\displaystyle \\sum\\_{j=1}^p                     | \\beta\\_j                                                                                      | ) (L₁ norm)                     | Can drive some coefficients exactly to zero → sparse (feature selection). |\n","| **Elastic Net** | (\\displaystyle \\alpha\\sum\\_j                     | \\beta\\_j                                                                                      | ;+;(1-\\alpha)\\sum\\_j\\beta\\_j^2) | Combines L₁ and L₂: both shrinks and selects.                             |\n","\n","* **$\\lambda$** controls the overall strength of regularization.\n","* **$\\alpha \\in [0,1]$** in Elastic Net tunes the mix between Lasso ($\\alpha=1$) and Ridge ($\\alpha=0$).\n","\n","---\n","\n","## 2. Geometric Interpretation\n","\n","* **Ridge (L₂)**⏤The constraint $\\sum \\beta_j^2 \\le t$ is a circle/ellipse in coefficient space. The solution “touches” the circle at a point where coefficients are small but typically nonzero.\n","* **Lasso (L₁)**⏤The constraint $\\sum |\\beta_j| \\le t$ is a diamond (a rotated square). Its corners lie on axes—solutions often occur at those corners, setting some $\\beta_j=0$.\n","* **Elastic Net**⏤The feasible region is the intersection (or convex combination) of the diamond and circle, blending sparsity and shrinkage.\n","\n","---\n","\n","## 3. When to Use Which\n","\n","| Criterion                       | Ridge                           | Lasso                                                          | Elastic Net                                      |\n","| ------------------------------- | ------------------------------- | -------------------------------------------------------------- | ------------------------------------------------ |\n","| **Number of predictors**        | When $p \\ll n$                  | When many predictors but expect only a few truly relevant ones | When $p > n$ or predictors are highly correlated |\n","| **Feature selection needed?**   | No                              | Yes                                                            | Yes, with stability                              |\n","| **Highly correlated features?** | Handles by distributing weights | Can arbitrarily pick one and ignore others                     | Groups correlated features together              |\n","| **Model interpretability**      | Moderate                        | High (sparser)                                                 | Moderate–high                                    |\n","\n","* **Ridge** is often preferred if you believe *all* features carry some signal, or when multicollinearity is severe.\n","* **Lasso** works well when you expect a *small subset* of features to be predictive.\n","* **Elastic Net** is a “best of both worlds” when you have many correlated features and still want sparsity but more stability than Lasso alone.\n","\n","---\n","\n","## 4. Objective Functions (Linear Regression Example)\n","\n","1. **Ridge**\n","\n","   $$\n","   \\min_{\\beta_0,\\mathbf{β}}\n","   \\;\\frac{1}{n}\\sum_{i=1}^n \\bigl(y_i - \\beta_0 - \\mathbf{x}_i^\\top\\mathbf{β}\\bigr)^2\n","   \\;+\\;\\lambda\\sum_{j=1}^p \\beta_j^2\n","   $$\n","\n","2. **Lasso**\n","\n","   $$\n","   \\min_{\\beta_0,\\mathbf{β}}\n","   \\;\\frac{1}{n}\\sum_{i=1}^n \\bigl(y_i - \\beta_0 - \\mathbf{x}_i^\\top\\mathbf{β}\\bigr)^2\n","   \\;+\\;\\lambda\\sum_{j=1}^p |\\beta_j|\n","   $$\n","\n","3. **Elastic Net**\n","\n","   $$\n","   \\min_{\\beta_0,\\mathbf{β}}\n","   \\;\\frac{1}{n}\\sum_{i=1}^n \\bigl(y_i - \\beta_0 - \\mathbf{x}_i^\\top\\mathbf{β}\\bigr)^2\n","   \\;+\\;\\lambda\\Bigl[\\alpha\\sum_{j=1}^p |\\beta_j|\n","             \\;+\\;(1-\\alpha)\\sum_{j=1}^p \\beta_j^2\\Bigr]\n","   $$\n","\n","---\n","\n","## 5. Computational Aspects\n","\n","* **Ridge** has a closed-form solution via modified normal equations:\n","  $\\;(X^\\top X + \\lambda I)\\mathbf{β} = X^\\top \\mathbf{y}$.\n","* **Lasso** lacks a closed form; requires coordinate descent, subgradient methods, or specialized algorithms (e.g., LARS).\n","* **Elastic Net** also uses coordinate descent, but with updates that account for both L₁ and L₂ penalties.\n","\n","---\n","\n","### Bottom Line\n","\n","* **Ridge** — shrinks all coefficients smoothly; no zeros.\n","* **Lasso** — promotes sparsity; can zero out irrelevant features.\n","* **Elastic Net** — balances shrinkage and sparsity; especially powerful when predictors outnumber samples or are highly correlated.\n","\n","---"],"metadata":{"id":"1XUUEF5kA8xK"}},{"cell_type":"markdown","source":["7.  When should we use Elastic Net instead of Lasso or Ridge?\n","- Use **Elastic Net** when you want the benefits of both Lasso (sparsity) and Ridge (stability) but neither pure L₁ nor pure L₂ does the job alone. In practice, reach for Elastic Net in these situations:\n","\n","1. **Highly Correlated Predictors**\n","\n","   * **Problem**: Lasso will tend to pick one variable out of a correlated group and ignore the rest; Ridge will shrink them all equally but keep them nonzero.\n","   * **Elastic Net Benefit**: Tends to select (and shrink) groups of correlated variables together, giving you a more stable, interpretable model when features cluster.\n","\n","2. **“p ≫ n” Scenarios (More Features than Samples)**\n","\n","   * **Problem**: Lasso can select at most $n$ variables before it saturates, limiting its sparsity when $p$ (features) ≫ $n$ (samples).\n","   * **Elastic Net Benefit**: Because of its L₂ component, it can include more than $n$ predictors, while still encouraging sparsity via L₁.\n","\n","3. **Desire for Both Sparsity and Shrinkage**\n","\n","   * **Problem**: You want a sparse model (for interpretability or cost reasons) but also don’t want the unselected features’ coefficients to explode or the selected ones to be overly large.\n","   * **Elastic Net Benefit**: L₁ drives many coefficients exactly to zero; L₂ gently shrinks the remaining nonzero coefficients toward zero, improving numerical stability and generalization.\n","\n","4. **When Pure Methods Underperform**\n","\n","   * **Problem**: Cross-validation shows that neither a pure Lasso nor a pure Ridge penalty gives the best out-of-sample performance.\n","   * **Elastic Net Benefit**: By tuning two hyperparameters—overall strength $\\lambda$ and mixing ratio $\\alpha\\in[0,1]$—you can interpolate between Lasso ($\\alpha=1$) and Ridge ($\\alpha=0$) to find the sweet spot.\n","\n","---\n","\n","### How to Implement\n","\n","* **Objective**\n","\n","  $$\n","    \\min_{\\beta_0,\\beta}\n","    \\frac{1}{n}\\sum_{i=1}^n\n","      \\bigl(y_i - \\beta_0 - x_i^\\top\\beta\\bigr)^2\n","    \\;+\\;\\lambda\\Bigl[\\alpha\\lVert\\beta\\rVert_1 \\;+\\;(1-\\alpha)\\lVert\\beta\\rVert_2^2\\Bigr].\n","  $$\n","* **Tuning**\n","\n","  1. **$\\lambda$** controls overall penalty strength.\n","  2. **$\\alpha$** controls the mix:\n","\n","     * $\\alpha=1$: pure Lasso\n","     * $\\alpha=0$: pure Ridge\n","     * $0<\\alpha<1$: Elastic Net\n","  3. Use **nested cross-validation** (grid search or randomized search) to find the best $(\\lambda,\\alpha)$ pair.\n","\n","---\n","\n","### Rule-of-Thumb\n","\n","* If you know your features are largely uncorrelated and you want automatic variable selection → **Lasso** may suffice.\n","* If you know all features contribute some signal and are worried about multicollinearity → **Ridge** may suffice.\n","* Otherwise, especially when correlation structure is unknown or mixed and you want both feature selection and robust shrinkage → **Elastic Net** is the most flexible choice.\n","\n","---"],"metadata":{"id":"kYDM2nc2BXWA"}},{"cell_type":"markdown","source":["8.  What is the impact of the regularization parameter (λ) in Logistic Regression?\n","- The regularization parameter $\\lambda$ in logistic regression controls the strength of the penalty on your model’s coefficients. Tuning $\\lambda$ lets you trade off bias and variance, and directly affects how “complex” your model can become:\n","\n","---\n","\n","## 1. Coefficient Shrinkage\n","\n","* **Large $\\lambda$**\n","\n","  * Heavier penalty → pushes $\\beta_j$ closer to zero.\n","  * Model becomes **simpler** (small weights) → less capable of fitting nuanced patterns.\n","* **Small $\\lambda$**\n","\n","  * Lighter penalty → $\\beta_j$ can take larger values.\n","  * Model becomes **more flexible** → can fit more complex relationships.\n","\n","---\n","\n","## 2. Bias–Variance Trade-off\n","\n","| $\\lambda$ size | Variance | Bias     | Overfitting Risk | Underfitting Risk |\n","| -------------- | -------- | -------- | ---------------- | ----------------- |\n","| **Very small** | High     | Low      | ✔️ Likely        | ❌ Unlikely        |\n","| **Moderate**   | Moderate | Moderate | ✔️/❌ Balanced    | ✔️/❌ Balanced     |\n","| **Very large** | Low      | High     | ❌ Unlikely       | ✔️ Likely         |\n","\n","* **As $\\lambda ↑$** → variance ↓, bias ↑\n","* **As $\\lambda ↓$** → variance ↑, bias ↓\n","\n","---\n","\n","## 3. Numerical Stability & Multicollinearity\n","\n","* **High $\\lambda$**\n","\n","  * Mitigates coefficient explosion when predictors are highly correlated.\n","  * Makes the Hessian (in Newton-type solvers) better conditioned → more stable convergence.\n","* **Low $\\lambda$**\n","\n","  * May suffer from unstable estimates if features are collinear or ill-scaled.\n","\n","---\n","\n","## 4. Interpretation & Sparsity\n","\n","* **With L₂ regularization (Ridge-style)**\n","\n","  * $\\lambda$ simply scales how “smoothly” coefficients shrink; none go exactly to zero.\n","* **With L₁ regularization (Lasso-style)**\n","\n","  * Larger $\\lambda$ not only shrinks coefficients but can drive some exactly to zero → feature selection.\n","* **Elastic Net**\n","\n","  * $\\lambda$ controls overall shrinkage, while the mixing parameter $\\alpha$ dictates L₁ vs. L₂ behavior.\n","\n","---\n","\n","## 5. Selecting $\\lambda$\n","\n","* **Cross-Validation**\n","\n","  * Sweep over a range (e.g. $\\lambda\\in[10^{-4},10^2]$ on a log scale).\n","  * Choose the $\\lambda$ that yields the best validation metric (e.g., AUC, accuracy, log-loss).\n","* **Practical tip**\n","\n","  * If you see your model overfitting (training accuracy ≫ validation accuracy), try **increasing** $\\lambda$.\n","  * If your model underfits (both accuracies low), try **decreasing** $\\lambda$.\n","\n","---\n","\n","### Bottom Line\n","\n","$\\lambda$ is your “knob” for model complexity:\n","\n","* **Turn up ($\\lambda↑$)** to simplify your model, reduce overfitting, and gain stability at the cost of possibly missing subtle patterns.\n","* **Turn down ($\\lambda↓$)** to allow richer fits (but beware of overfitting and numerical issues).\n","\n","Finding the right $\\lambda$ via systematic validation is key to robust, well-generalizing logistic regression models.\n","\n","---"],"metadata":{"id":"KoeERo1FB8yn"}},{"cell_type":"markdown","source":["9.  What are the key assumptions of Logistic Regression?\n","- Logistic regression relies on several core assumptions to produce valid, reliable estimates and inferences. While it’s more flexible than linear regression in some respects, you still need to check that these conditions are reasonably met:\n","\n","---\n","\n","## 1. Correctly Specified Functional Form\n","\n","* **Linearity of the Logit**\n","  The model assumes that the log-odds of the outcome is a *linear* function of the predictors:\n","\n","  $$\n","    \\ln\\frac{P(y=1)}{P(y=0)} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p.\n","  $$\n","\n","  If you have a nonlinear relationship, you’ll need to transform features (e.g., polynomials, splines) or add interaction terms.\n","\n","---\n","\n","## 2. Independence of Observations\n","\n","* Each case (row) must be independent of the others.\n","* Violations occur in clustered or longitudinal data; in those cases, consider mixed-effects logistic regression or generalized estimating equations (GEE).\n","\n","---\n","\n","## 3. Absence of (Near) Perfect Multicollinearity\n","\n","* Predictors should not be *perfectly* (or nearly perfectly) correlated.\n","* High multicollinearity inflates standard errors and destabilizes coefficient estimates.\n","* Check variance inflation factors (VIFs) and, if needed, remove or combine collinear features.\n","\n","---\n","\n","## 4. Sufficient Sample Size & Event Rate\n","\n","* Unlike linear regression, logistic regression’s likelihood estimates can be biased or unstable with small samples or too few events.\n","* A common rule of thumb is **at least 10 events per predictor** (i.e., if you have 5 predictors, aim for ≥50 cases of the minority class).\n","\n","---\n","\n","## 5. No (or Limited) Overdispersion\n","\n","* In binary logistic regression, variance is determined by the mean: $p(1-p)$.\n","* If you observe more variability than the model allows (overdispersion), consider alternatives like quasi-binomial models or adding random effects.\n","\n","---\n","\n","## 6. No Complete Separation\n","\n","* **Complete separation** occurs if a predictor (or combination) perfectly predicts the outcome (e.g., all $y=1$ when $x>5$ and all $y=0$ when $x\\le5$).\n","* This causes the maximum-likelihood estimates to diverge (coefficients tend to ±∞).\n","* Remedies include penalized (regularized) logistic regression, Firth’s bias reduction, or collapsing/separating levels.\n","\n","---\n","\n","## 7. Predictors Measured Without (Substantial) Error\n","\n","* Measurement error in predictors can bias coefficient estimates and degrade model performance.\n","* If error is nonignorable, consider measurement-error models or instrument variables.\n","\n","---\n","\n","### Checking & Addressing Assumptions\n","\n","* **Linearity of logit**: Use Box-Tidwell test or plot residuals vs. predictors.\n","* **Multicollinearity**: Compute VIFs; drop or combine features with VIF > 5–10.\n","* **Sample size**: Ensure a minimum events-per-variable; if too small, simplify the model.\n","* **Separation**: Fit a penalized model (e.g., L₂-regularized logistic regression) or use Firth’s method.\n","* **Independence**: If clustered, switch to mixed models or GEE.\n","\n","---\n","\n","Ensuring these assumptions are reasonably satisfied will help your logistic regression model be stable, interpretable, and generalizable to new data.\n","\n","---"],"metadata":{"id":"FITMWJVfCLKV"}},{"cell_type":"markdown","source":["10.  What are some alternatives to Logistic Regression for classification tasks?\n","- Here are several widely-used alternatives to logistic regression for binary (or multiclass) classification, along with their key characteristics and when you might choose them:\n","\n","---\n","\n","## 1. Decision Trees\n","\n","* **How it works**: Recursively split the feature space based on values that best separate classes (e.g., information gain or Gini impurity).\n","* **Pros**:\n","\n","  * Captures non-linear relationships without feature engineering.\n","  * Easily interpretable (“if–then” rules).\n","* **Cons**:\n","\n","  * Prone to overfitting (can grow very deep).\n","* **When to use**: Quick baseline, interpretability, data with clear hierarchical splits.\n","\n","---\n","\n","## 2. Random Forests\n","\n","* **How it works**: An ensemble of decision trees, each trained on a bootstrap sample and random subset of features; predictions are averaged (or majority-voted).\n","* **Pros**:\n","\n","  * Robust to overfitting.\n","  * Handles high-dimensional and noisy data well.\n","* **Cons**:\n","\n","  * Less interpretable than a single tree.\n","* **When to use**: General-purpose classifier, especially when you need strong accuracy without heavy tuning.\n","\n","---\n","\n","## 3. Gradient Boosting Machines (e.g., XGBoost, LightGBM)\n","\n","* **How it works**: Sequentially builds trees, each one correcting errors of the previous ensemble, optimizing a differentiable loss (e.g., logistic loss).\n","* **Pros**:\n","\n","  * Often state-of-the-art performance on tabular data.\n","  * Offers regularization and handling of missing values.\n","* **Cons**:\n","\n","  * More sensitive to hyperparameters; longer training times.\n","* **When to use**: When you need maximum predictive power on structured data and can afford tuning.\n","\n","---\n","\n","## 4. Support Vector Machines (SVM)\n","\n","* **How it works**: Finds a hyperplane that maximizes the margin between classes; can use kernel functions (e.g., RBF, polynomial) for non-linear separation.\n","* **Pros**:\n","\n","  * Effective in high-dimensional spaces.\n","  * Robust to overfitting (especially with proper $C$ and kernel choice).\n","* **Cons**:\n","\n","  * Not easily scalable to very large datasets.\n","  * Less interpretable, requires careful kernel/tuning.\n","* **When to use**: Medium-sized datasets, complex class boundaries, text classification.\n","\n","---\n","\n","## 5. k-Nearest Neighbors (k-NN)\n","\n","* **How it works**: Classifies a point by majority vote among its $k$ closest training examples in feature space.\n","* **Pros**:\n","\n","  * Simple, no explicit training phase.\n","  * Captures arbitrary decision boundaries given enough data.\n","* **Cons**:\n","\n","  * Prediction is costly for large datasets.\n","  * Sensitive to feature scaling and choice of $k$.\n","* **When to use**: Small-to-medium datasets, when you suspect local structure matters.\n","\n","---\n","\n","## 6. Naïve Bayes\n","\n","* **How it works**: Applies Bayes’ theorem under the (strong) assumption that features are conditionally independent given the class.\n","* **Pros**:\n","\n","  * Extremely fast to train and predict.\n","  * Works surprisingly well on text (e.g., spam filtering).\n","* **Cons**:\n","\n","  * The independence assumption rarely holds exactly.\n","* **When to use**: Text classification, high-dimensional sparse data.\n","\n","---\n","\n","## 7. Neural Networks\n","\n","* **How it works**: Composed of layers of interconnected “neurons” with learnable weights; can learn arbitrary non-linear mappings.\n","* **Pros**:\n","\n","  * Highly flexible; state-of-the-art for images, speech, complex patterns.\n","* **Cons**:\n","\n","  * Requires large amounts of data.\n","  * Longer training times, many hyperparameters.\n","* **When to use**: Large datasets with complex feature interactions (e.g., image, speech, deep tabular learning).\n","\n","---\n","\n","## 8. Quadratic Discriminant Analysis (QDA) / Linear Discriminant Analysis (LDA)\n","\n","* **How it works**: Models class-conditional feature distributions as Gaussians; uses Bayes’ theorem to classify.\n","* **Pros**:\n","\n","  * Fast to train.\n","  * LDA works well when classes are roughly Gaussian with shared covariance.\n","* **Cons**:\n","\n","  * Performance degrades if Gaussian assumption is violated.\n","* **When to use**: When you believe feature distributions are (approximately) Gaussian and sample-to-feature ratio is reasonable.\n","\n","---\n","\n","### Choosing Among Them\n","\n","* **Interpretability & simplicity**: Decision Trees, LDA, Naïve Bayes\n","* **Accuracy on tabular data**: Random Forest, Gradient Boosting\n","* **High-dimensional text/image**: SVM, Neural Networks\n","* **Small datasets / local patterns**: k-NN\n","\n","Ultimately, your choice depends on your data size, feature characteristics, interpretability needs, and computational budget. It’s often worth trying several and comparing via cross-validation.\n","\n","---"],"metadata":{"id":"vw1mPS3MCYbS"}},{"cell_type":"markdown","source":["11. What are Classification Evaluation Metrics?\n","- Here’s a rundown of the most common metrics used to evaluate classification models, what they measure, and when to use them:\n","\n","---\n","\n","## 1. Confusion Matrix\n","\n","A **confusion matrix** lays out the four possible outcomes for binary classification (can be generalized to multiclass):\n","\n","|                     | **Predicted Positive** | **Predicted Negative** |\n","| ------------------- | ---------------------- | ---------------------- |\n","| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n","| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n","\n","Every other metric below is built from these four counts.\n","\n","---\n","\n","## 2. Accuracy\n","\n","$$\n","\\mathrm{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n","$$\n","\n","* **What it measures**: Overall fraction of correctly classified samples.\n","* **When to use**: When classes are balanced and all errors are equally costly.\n","\n","---\n","\n","## 3. Precision (Positive Predictive Value)\n","\n","$$\n","\\mathrm{Precision} = \\frac{TP}{TP + FP}\n","$$\n","\n","* **What it measures**: Of all instances the model predicted positive, how many were actually positive.\n","* **When to use**: When false positives are costly (e.g. spam filter marking good email as spam).\n","\n","---\n","\n","## 4. Recall (Sensitivity, True Positive Rate)\n","\n","$$\n","\\mathrm{Recall} = \\frac{TP}{TP + FN}\n","$$\n","\n","* **What it measures**: Of all actual positives, how many the model correctly identified.\n","* **When to use**: When false negatives are costly (e.g. missing a disease diagnosis).\n","\n","---\n","\n","## 5. Specificity (True Negative Rate)\n","\n","$$\n","\\mathrm{Specificity} = \\frac{TN}{TN + FP}\n","$$\n","\n","* **What it measures**: Of all actual negatives, how many the model correctly identifies.\n","* **When to use**: Alongside recall when you need to understand both sides of class imbalance.\n","\n","---\n","\n","## 6. F₁-Score\n","\n","$$\n","F_1 = 2 \\,\\frac{\\text{Precision}\\,\\times\\,\\text{Recall}}{\\text{Precision} + \\text{Recall}}\n","$$\n","\n","* **What it measures**: Harmonic mean of precision and recall.\n","* **When to use**: When you want a single metric balancing precision and recall, especially under class imbalance.\n","\n","---\n","\n","## 7. ROC Curve & AUC\n","\n","* **ROC Curve**: Plots True Positive Rate (Recall) vs. False Positive Rate (1–Specificity) as you vary the classification threshold.\n","* **AUC (Area Under the Curve)**: Scalar summary of the ROC curve, between 0.5 (random) and 1.0 (perfect).\n","* **When to use**: To compare models’ discrimination ability regardless of threshold or class balance.\n","\n","---\n","\n","## 8. Precision–Recall Curve & Average Precision\n","\n","* **PR Curve**: Plots Precision vs. Recall as threshold varies.\n","* **Average Precision (AP)**: Area under the PR curve.\n","* **When to use**: In heavily imbalanced settings where the positive class is rare—PR curves focus on performance over positives.\n","\n","---\n","\n","## 9. Log-Loss (Cross-Entropy Loss)\n","\n","$$\n","\\text{LogLoss} = -\\frac{1}{N}\\sum_{i=1}^N \\bigl[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)\\bigr]\n","$$\n","\n","* **What it measures**: How “surprised” the model is by the true labels given its predicted probabilities.\n","* **When to use**: When you care about the quality of predicted probabilities, not just hard classifications.\n","\n","---\n","\n","## 10. Cohen’s Kappa\n","\n","$$\n","\\kappa = \\frac{p_o - p_e}{1 - p_e}\n","$$\n","\n","where $p_o$ is observed accuracy and $p_e$ is expected accuracy by chance.\n","\n","* **What it measures**: Agreement between predictions and true labels, adjusted for chance agreement.\n","* **When to use**: When you want to account for the possibility of correct predictions occurring by chance.\n","\n","---\n","\n","### Choosing Metrics\n","\n","* **Balanced classes & equal error cost** → Accuracy\n","* **Imbalanced classes** → Precision/Recall, F₁, PR-AUC\n","* **Need probability estimates** → Log-Loss, ROC-AUC\n","* **Cost asymmetry** → Pick Precision or Recall based on which error (FP vs FN) you penalize more.\n","\n","Often you’ll report several of these together (e.g., confusion matrix + accuracy + F₁ + ROC-AUC) to get a full picture of model performance.\n","\n","---"],"metadata":{"id":"INzGo4tGCYAS"}},{"cell_type":"markdown","source":["12.  How does class imbalance affect Logistic Regression?\n","- Class imbalance—where one class (usually the “negative” or majority class) vastly outnumbers the other (“positive” or minority)—can materially degrade the performance of a standard logistic‐regression model in several ways:\n","\n","---\n","\n","## 1. Biased Parameter Estimates & Decision Boundary\n","\n","* **Dominance of Majority Class in Likelihood**\n","  Logistic regression fits parameters by maximizing the overall likelihood (or minimizing cross‐entropy). If 95% of examples are class 0, the model can achieve low loss by simply predicting “class 0” almost always. This pulls the learned decision boundary toward class 1 instances, making it harder to correctly identify the minority class.\n","* **Skewed Probabilities**\n","  The estimated probability $\\hat p$ tends to be calibrated around the majority class prevalence. Even genuinely positive examples may get low $\\hat p$ because the model “learns” that positive labels are rare.\n","\n","---\n","\n","## 2. Poor Recall (Sensitivity) on Minority Class\n","\n","* **Thresholding Issue**\n","  If you use the usual 0.5 cutoff, a minority example often falls below it—even if the model correctly assigns it a higher risk relative to others—leading to many false negatives (FN).\n","* **Metric Deception**\n","  Overall accuracy may remain high (e.g., 95% by always guessing class 0) while recall for the minority class plummets, masking real performance problems.\n","\n","---\n","\n","## 3. Instability & Overfitting on Limited Minority Data\n","\n","* **High Variance in Minority Region**\n","  With few positive cases, the model has little data to learn the shape of the decision boundary in that region. Coefficient estimates that heavily affect minority predictions can become unstable (high variance), especially if features are weak predictors for the rare class.\n","* **Overfitting Risk**\n","  If you oversample the minority class naively (e.g., by simple duplication), the model can overfit to those repeated points.\n","\n","---\n","\n","## 4. Remedies & Best Practices\n","\n","| Strategy            | How It Helps                                                                                                                                                        |\n","| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| **Class Weighting** | Increase the penalty for misclassifying minority cases by setting `weight_pos > 1` in the loss; effectively “tells” the model to pay more attention to rare events. |\n","| **Resampling**      |                                                                                                                                                                     |\n","\n","* **Undersampling**: Randomly drop majority examples to balance classes.\n","* **Oversampling**: Duplicate or synthetically generate (e.g., SMOTE) minority examples.\n","  Balances the training distribution so the model sees more positives. |\n","  \\| **Threshold Adjustment**      | Tune the decision threshold away from 0.5 (e.g., choose the threshold that maximizes F₁ or a cost‐sensitive metric on validation data). |\n","  \\| **Use Appropriate Metrics**   | Optimize and evaluate using recall, precision–recall AUC, F₁‐score, or cost‐weighted errors rather than raw accuracy. |\n","  \\| **Ensemble Methods**          | Bagging/boosting (e.g., balanced random forests, gradient boosting with class‐weighted loss) can be more robust to imbalance. |\n","\n","---\n","\n","## 5. Implementation Example (Scikit-Learn)\n","\n","```python\n","from sklearn.linear_model import LogisticRegression\n","\n","# 1. Class weighting\n","model = LogisticRegression(class_weight='balanced')\n","\n","# 2. Custom weighting\n","model = LogisticRegression(class_weight={0:1, 1:10})\n","\n","# 3. Resampling with imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","X_res, y_res = SMOTE().fit_resample(X, y)\n","model.fit(X_res, y_res)\n","```\n","\n","---\n","\n","### Bottom Line\n","\n","Class imbalance in logistic regression causes the model to gravitate toward the majority class, yielding low sensitivity on the minority. You must proactively rebalance the training process (via weighting or resampling), adjust your decision threshold, and evaluate with metrics designed for rare‐event scenarios to build a reliable classifier.\n","\n","---"],"metadata":{"id":"1-eFzZTvC3hr"}},{"cell_type":"markdown","source":["13.  What is Hyperparameter Tuning in Logistic Regression?\n","- Hyperparameter tuning in logistic regression is the process of selecting the best settings for the model’s hyperparameters—parameters that are not learned during training but set before the learning process begins—to optimize performance (e.g., accuracy, AUC, F₁-score) on unseen data.\n","\n","---\n","\n","## 1. Common Hyperparameters in Logistic Regression\n","\n","| Hyperparameter | Description                                                                                  |\n","| -------------- | -------------------------------------------------------------------------------------------- |\n","| **C**          | Inverse of regularization strength ($\\lambda = 1/C$). Smaller $C$ ⇒ stronger regularization. |\n","| **penalty**    | Type of regularization to apply:                                                             |\n","\n","* `l2` (Ridge)\n","* `l1` (Lasso; requires a solver that supports it)\n","* `elasticnet` (combined L₁/L₂) |\n","  \\| **solver**          | Optimization algorithm, e.g.:\n","* `liblinear` (good for small datasets & L₁)\n","* `saga` (scales to large datasets, supports L₁/L₂/elasticnet)\n","* `lbfgs` (default, L₂ only) |\n","  \\| **class\\_weight**    | Adjusts the loss to pay more/less attention to each class (e.g., `balanced` or custom dict).             |\n","  \\| **max\\_iter**        | Maximum number of iterations for the solver to converge.                                                  |\n","  \\| **l1\\_ratio**        | When `penalty='elasticnet'`, controls mix between L₁ and L₂ (0 ⇒ pure L₂, 1 ⇒ pure L₁).                     |\n","\n","---\n","\n","## 2. Why Tune Hyperparameters?\n","\n","* **Bias–Variance Trade-off**: E.g., too little regularization (large $C$) can overfit; too much (small $C$) can underfit.\n","* **Algorithm Stability**: Choice of solver and `max_iter` can affect convergence and runtime.\n","* **Class Imbalance Handling**: Adjusting `class_weight` can improve minority‐class performance.\n","\n","---\n","\n","## 3. Tuning Strategies\n","\n","1. **Grid Search**\n","\n","   * Define a discrete grid of hyperparameter values (e.g., `C = [0.01, 0.1, 1, 10]`, `penalty = ['l1','l2']`).\n","   * Exhaustively evaluate each combination via cross-validation.\n","   * Select the combination with the best average validation score.\n","\n","2. **Random Search**\n","\n","   * Sample random combinations from specified distributions (e.g., log-uniform for $C$).\n","   * More efficient when the search space is large.\n","\n","3. **Bayesian Optimization**\n","\n","   * Model the validation score as a function of hyperparameters and iteratively explore the space to find the optimum more efficiently than random search.\n","\n","4. **Automated Tools**\n","\n","   * Libraries like **scikit-learn’s** `GridSearchCV` / `RandomizedSearchCV`, or **Optuna**, **Hyperopt**, **Ray Tune**.\n","\n","---\n","\n","## 4. Example with Scikit-Learn\n","\n","```python\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","    'penalty': ['l2', 'l1'],\n","    'solver': ['liblinear'],\n","    'class_weight': [None, 'balanced']\n","}\n","\n","grid = GridSearchCV(\n","    LogisticRegression(max_iter=1000),\n","    param_grid,\n","    cv=5,\n","    scoring='roc_auc'\n",")\n","grid.fit(X_train, y_train)\n","\n","print(\"Best params:\", grid.best_params_)\n","print(\"Best AUC:\", grid.best_score_)\n","```\n","\n","---\n","\n","## 5. Best Practices\n","\n","* **Scale Features**: When using L₁/L₂ penalties, standardize or normalize features so the regularization treats all coefficients fairly.\n","* **Use Nested Cross-Validation**: To get an unbiased estimate of generalization performance when tuning hyperparameters.\n","* **Monitor Overfitting**: Compare training vs. validation scores across hyperparameter settings.\n","* **Limit Search Space**: Start with a coarse grid, then refine around promising regions.\n","\n","---\n","\n","**In summary**, hyperparameter tuning systematically searches for the best configuration (regularization strength, penalty type, solver settings, etc.) to balance bias and variance, ensure convergence, and align the model with your performance goals.\n","\n","---"],"metadata":{"id":"fYyU58ovDKe_"}},{"cell_type":"markdown","source":["14.  What are different solvers in Logistic Regression? Which one should be used?\n","- Here’s a rundown of the most common optimization **solvers** you’ll encounter in logistic-regression implementations (e.g., scikit-learn), how they work at a high level, and guidance on when to pick each:\n","\n","| **Solver**    | **Algorithm & Penalties Supported**                                                    | **Pros**                                                                           | **Cons**                                                                    | **When to Use**                                                                                     |\n","| ------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n","| **liblinear** | Coordinate descent (for L1) & trust-region (for L2)                                    | - Works well on small to medium datasets<br>- Supports L1 & L2                     | - Only binary classification (no native “multinomial” mode)                 | – When you need L1 (sparse) or simple L2 on smaller problems<br>– If your dataset is modest in size |\n","| **newton-cg** | Newton’s method with conjugate-gradient inner solves; L2 only                          | - Fast convergence near optimum for smooth losses<br>- Handles multiclass natively | - Requires computing Hessian-vector products (can be slow on high-dim data) | – When you have medium-sized data, no L1 regularization, and want a reliable multiclass solver      |\n","| **lbfgs**     | Limited-memory BFGS quasi-Newton method; L2 only                                       | - Excellent general-purpose solver<br>- Handles multiclass out of the box          | - Doesn’t support L1 or elastic-net                                         | – Default choice for most problems with L2 penalty<br>– Good “first try” for multiclass tasks       |\n","| **sag**       | Stochastic Average Gradient (a variance-reduced SGD)                                   | - Very fast on large, smooth problems<br>- Supports only L2                        | - Requires feature scaling<br>- No L1                                       | – When you have **very large** datasets and only need L2<br>– Features are scaled/normalized        |\n","| **saga**      | “SAGA” variant of SAG (variance-reduced); supports L1, L2, elastic-net, and multiclass | - Scales to large data<br>- Supports all penalties (L1, L2, elastic-net)           | - Needs more iterations to converge than lbfgs on small data                | – Large datasets where you need L1/elastic-net<br>– Sparse solutions on big tables                  |\n","\n","---\n","\n","### Solver Selection Guidelines\n","\n","1. **Penalty Type**\n","\n","   * **L1 or Elastic-Net Required** → `liblinear` (binary-only) or **`saga`** (binary & multiclass).\n","   * **L2 Only** → Any solver; prefer `lbfgs` or `newton-cg` for robustness, `sag`/`saga` for speed on large data.\n","\n","2. **Dataset Size**\n","\n","   * **Small to Medium** (say, <100 k samples):\n","\n","     * Default to **`lbfgs`** (multiclass & fast convergence).\n","     * If you need sparsity, use `liblinear`.\n","   * **Large** (>100 k samples):\n","\n","     * **`sag`/`saga`** shine, especially with well-scaled features.\n","     * `saga` if you also need L1/elastic-net.\n","\n","3. **Multiclass Support**\n","\n","   * **Native Multiclass**: `lbfgs`, `newton-cg`, `sag`, `saga`\n","   * **Binary Only**: `liblinear`\n","\n","4. **Convergence & Stability**\n","\n","   * **Quick & Stable**: `lbfgs` often converges in fewer iterations.\n","   * **Memory Constraints**: `lbfgs` is “limited-memory” but still needs to store a few vectors; `sag`/`saga` use less memory per iteration.\n","\n","---\n","\n","### Example (scikit-learn)\n","\n","```python\n","from sklearn.linear_model import LogisticRegression\n","\n","# 1. Default (L2, multiclass via lbfgs)\n","model = LogisticRegression(solver='lbfgs', max_iter=1000)\n","\n","# 2. L1 penalty on a large dataset\n","model = LogisticRegression(\n","    penalty='l1',\n","    solver='saga',\n","    l1_ratio=0.5,    # for elastic-net\n","    C=1.0,\n","    max_iter=2000\n",")\n","\n","# 3. Fast L2 on huge data\n","model = LogisticRegression(\n","    penalty='l2',\n","    solver='sag',\n","    class_weight='balanced',\n","    max_iter=1000\n",")\n","```\n","\n","---\n","\n","**Bottom line:**\n","\n","* If you’re unsure, start with **`lbfgs`** for L2-regularized problems—its blend of speed, stability, and native multiclass support makes it a solid default.\n","* If you need L1 or elastic-net (for feature selection), go with **`saga`**.\n","* For very large datasets where only L2 is needed and features are scaled, **`sag`** (or `saga`) will typically train fastest.\n","\n","---"],"metadata":{"id":"6QG_lZwtDZ9R"}},{"cell_type":"markdown","source":["15.  How is Logistic Regression extended for multiclass classification?\n","- Logistic regression naturally handles binary outcomes, but many real‐world tasks involve more than two classes. There are two main strategies for extending it:\n","\n","---\n","\n","## 1. One-vs-Rest (OvR, a.k.a. One-vs-All)\n","\n","1. **Idea**\n","\n","   * Train **K** separate binary classifiers, one for each class $k=1,\\dots,K$.\n","   * For classifier $k$, label all examples of class $k$ as “positive” ($y=1$) and all other examples as “negative” ($y=0$).\n","\n","2. **Prediction**\n","\n","   * Given a new input $\\mathbf{x}$, compute each classifier’s probability\n","\n","     $$\n","       p_k = P(y=k \\mid \\mathbf{x})\n","       = \\sigma\\bigl(\\beta_{0}^{(k)} + \\boldsymbol\\beta^{(k)T}\\mathbf{x}\\bigr).\n","     $$\n","   * Choose the class with the highest probability:\n","     $\\displaystyle\\hat y = \\arg\\max_{k} p_k.$\n","\n","3. **Pros / Cons**\n","\n","   * **Pros**: Simple, leverages any binary‐only solver, scales to large class counts.\n","   * **Cons**: Decision boundaries can be inconsistent (regions where no classifier is confident, or multiple are equally confident).\n","\n","---\n","\n","## 2. Multinomial (“Softmax”) Logistic Regression\n","\n","1. **Model**\n","\n","   * Instead of $K$ independent binary models, fit a single model with a weight vector $\\boldsymbol\\beta_k$ for each class $k$.\n","   * Compute the raw score for class $k$:\n","\n","     $$\n","       z_k = \\beta_{0}^{(k)} + \\boldsymbol\\beta_k^{T}\\mathbf{x}.\n","     $$\n","   * Convert scores to probabilities via the **softmax** function:\n","\n","     $$\n","       P(y = k \\mid \\mathbf{x})\n","       = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)}.\n","     $$\n","\n","2. **Loss Function**\n","\n","   * The cost is the **multinomial cross-entropy** (generalizing binary log-loss):\n","\n","     $$\n","       J(\\{\\boldsymbol\\beta_k\\})\n","       = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K\n","         \\mathbb{1}\\{y^{(i)}=k\\}\\,\\log P(y^{(i)}=k \\mid \\mathbf{x}^{(i)}).\n","     $$\n","   * This is optimized jointly over all $\\{\\beta_{0}^{(k)},\\boldsymbol\\beta_k\\}$.\n","\n","3. **Pros / Cons**\n","\n","   * **Pros**:\n","\n","     * Produces **consistent** class‐probability estimates that sum to 1.\n","     * Single optimization problem—often better statistical efficiency than independent OvR fits.\n","   * **Cons**:\n","\n","     * Slightly more complex solver requirements (must handle the coupled softmax objective).\n","     * Memory and compute scale with $K$, since you have $K$ weight vectors.\n","\n","---\n","\n","## 3. One-vs-One (OvO)\n","\n","* **Less common** for logistic regression but sometimes used when $K$ is small.\n","* Train $\\tfrac{K(K-1)}2$ binary classifiers, each discriminating between a pair of classes $(i,j)$.\n","* At prediction, each classifier votes for its preferred class; the class with most votes wins.\n","\n","---\n","\n","## 4. When to Use Which\n","\n","| Strategy    | Use when…                                                                                      |\n","| ----------- | ---------------------------------------------------------------------------------------------- |\n","| OvR         | — You only have a binary‐solver available<br/>— $K$ is large and you want $K$ independent fits |\n","| Multinomial | — You need well-calibrated probabilities summing to 1<br/>— You want a single, joint model     |\n","| OvO         | — $K$ is small (e.g., 3–5 classes)<br/>— You want to focus on pairwise separations             |\n","\n","---\n","\n","### In Practice (Scikit-Learn)\n","\n","* **OvR** is the default for `LogisticRegression` when `multi_class='ovr'`.\n","* **Softmax** is enabled with `multi_class='multinomial'` (requires a solver like `lbfgs`, `newton-cg`, or `saga`).\n","\n","```python\n","from sklearn.linear_model import LogisticRegression\n","\n","# One-vs-Rest\n","ovr = LogisticRegression(multi_class='ovr', solver='liblinear').fit(X, y)\n","\n","# Multinomial (Softmax)\n","soft = LogisticRegression(multi_class='multinomial', solver='lbfgs').fit(X, y)\n","```\n","\n","---\n","\n","By choosing between OvR, softmax multinomial, or even OvO, you can adapt logistic regression to virtually any multiclass classification problem.\n","\n","---"],"metadata":{"id":"so6Dx7sTDojb"}},{"cell_type":"markdown","source":["16. What are the advantages and disadvantages of Logistic Regression?\n","- **Logistic Regression** is a workhorse of binary (and—with extensions—multiclass) classification. It shines in many scenarios but also has clear limitations. Below is a concise overview.\n","\n","---\n","\n","## Advantages\n","\n","1. **Simplicity & Interpretability**\n","\n","   * **Coefficients as Log-Odds**: Each feature’s effect is directly interpretable as a change in log-odds (or odds ratio).\n","   * **Transparent Decision Boundary**: It’s easy to visualize and explain why a particular prediction was made.\n","\n","2. **Probabilistic Output**\n","\n","   * Produces well-calibrated probabilities ($0$ to $1$), not just class labels, enabling risk-based decision making.\n","\n","3. **Computational Efficiency**\n","\n","   * Training is fast (convex optimization), even on reasonably large datasets.\n","   * Prediction is trivial (just a dot-product plus sigmoid).\n","\n","4. **Baseline Performance**\n","\n","   * Often a strong first model for tabular data—serves as a reliable benchmark before moving to more complex methods.\n","\n","5. **Regularization & Sparsity**\n","\n","   * Easily incorporates L₁/L₂/elastic-net penalties to prevent overfitting and perform feature selection.\n","\n","6. **Less Data-Hungry**\n","\n","   * Requires far fewer data points to converge than many “deep” methods, especially when the number of features is moderate.\n","\n","---\n","\n","## Disadvantages\n","\n","1. **Linear Decision Boundary**\n","\n","   * Can only separate classes via a hyperplane (in feature space).\n","   * Poor fit if the true boundary is highly non-linear—needs manual feature engineering (polynomials, interactions, splines).\n","\n","2. **Sensitive to Multicollinearity**\n","\n","   * Highly correlated predictors inflate variances of coefficient estimates, making them unstable (though regularization helps).\n","\n","3. **Outliers Influence**\n","\n","   * Extreme feature values can unduly sway the fitted hyperplane unless you preprocess or regularize.\n","\n","4. **Assumes Correct Link Function**\n","\n","   * Presumes the logit link is appropriate; a badly misspecified link (or omitted non-linear terms) can bias results.\n","\n","5. **Not Robust to Overdispersion**\n","\n","   * In contexts where the binary‐outcome variance exceeds the Bernoulli assumption (e.g., clustered data), standard logistic regression underestimates uncertainty.\n","\n","6. **Limited Expressiveness**\n","\n","   * Cannot naturally capture complex feature interactions or hierarchical structures without extensive feature engineering.\n","\n","---\n","\n","### Quick Comparison\n","\n","| Aspect                   | Logistic Regression               | More Complex Models (e.g., Trees, SVMs, NNs)            |\n","| ------------------------ | --------------------------------- | ------------------------------------------------------- |\n","| **Interpretability**     | ★★★★★                             | ★★☆☆☆                                                   |\n","| **Training Speed**       | ★★★★★                             | ★★☆☆☆–★★★☆☆                                             |\n","| **Probabilistic Output** | ★★★★★                             | Depends (e.g., needs calibration for SVMs)              |\n","| **Non-linear Modeling**  | ★☆☆☆☆ (needs feature engineering) | ★★★★☆–★★★★★ (via kernels or network depth)              |\n","| **Data Requirements**    | Low                               | Often high (especially NNs)                             |\n","| **Feature Engineering**  | Moderate                          | Varies—sometimes minimal (trees), sometimes heavy (SVM) |\n","\n","---\n","\n","**Bottom Line:**\n","Use logistic regression when you need a fast, interpretable, probability-driven model and believe the classes are roughly linearly separable in some feature space (possibly after transformations). For highly non-linear problems or very complex interactions, consider more expressive methods—at the cost of interpretability and often more data.\n","\n","---"],"metadata":{"id":"rG5vQU45D3r8"}},{"cell_type":"markdown","source":["17. What are some use cases of Logistic Regression?\n","- Logistic regression is a versatile classifier that’s been widely adopted across many domains whenever you need a fast, interpretable model for predicting a binary (or—with slight modifications—multiclass) outcome. Here are some common real-world use cases:\n","\n","1. **Medical Diagnosis**\n","\n","   * **Disease Screening**: Estimating the probability that a patient has a condition (e.g., diabetes, heart disease) based on clinical measurements (blood pressure, glucose levels, BMI).\n","   * **Diagnostic Test Interpretation**: Combining test results (e.g., biomarkers, imaging scores) to predict presence vs. absence of a disease.\n","\n","2. **Credit & Risk Scoring**\n","\n","   * **Loan Default Prediction**: Predicting whether an applicant will default on a loan using financial history, income, employment status, credit utilization, etc.\n","   * **Fraud Detection**: Flagging potentially fraudulent transactions (binary: fraud vs. legitimate) based on transaction amount, location, device metadata.\n","\n","3. **Marketing & Customer Analytics**\n","\n","   * **Churn Prediction**: Identifying customers likely to cancel a subscription or service, using usage metrics, billing history, and support interactions.\n","   * **Response Modeling**: Estimating the likelihood a prospect will respond (yes/no) to a campaign or promotion, guiding targeted marketing spend.\n","\n","4. **Spam & Content Filtering**\n","\n","   * **Email Spam Detection**: Classifying incoming emails as spam vs. non-spam based on features like word frequencies, sender reputation, and header metadata.\n","   * **Comment Moderation**: Binary filtering of malicious or offensive content in user-generated text (e.g., comments, reviews).\n","\n","5. **Manufacturing & Quality Control**\n","\n","   * **Defect Detection**: Predicting whether a manufactured part will pass quality inspection based on sensor readings, machine settings, and raw material parameters.\n","   * **Preventive Maintenance**: Classifying equipment state as “needs maintenance” vs. “healthy” using vibration, temperature, and operational logs.\n","\n","6. **Credit Card & Banking**\n","\n","   * **Credit Card Approval**: Deciding whether to approve a new credit card application (approve vs. decline) based on applicant demographics, credit score, and existing liabilities.\n","   * **Loan Pre-qualification**: Quickly screening applicants into “pre-qualified” vs. “not pre-qualified” buckets before a full underwriting process.\n","\n","7. **Social Science & Survey Analysis**\n","\n","   * **Predicting Voting Behavior**: Modeling whether an individual will vote for a given candidate/party (yes/no) based on demographics, past voting history, and survey responses.\n","   * **Behavioral Studies**: Classifying participants into groups (e.g., adopter vs. non-adopter of a technology) based on attitudinal and socioeconomic variables.\n","\n","8. **Online Services & Recommendation Systems**\n","\n","   * **Ad Click-Through Rate (CTR)**: Estimating whether a user will click on an ad (click vs. no-click) given features like ad position, user profile, and context.\n","   * **Content Personalization**: Predicting binary engagement (watch vs. skip, download vs. ignore) on media platforms to tailor recommendations.\n","\n","---\n","\n","These scenarios leverage logistic regression’s strengths—speed, probabilistic outputs, and interpretability—to make data-driven binary decisions in critical applications.\n","\n","---"],"metadata":{"id":"FbuG5m5MEDIC"}},{"cell_type":"markdown","source":["18.  What is the difference between Softmax Regression and Logistic Regression?\n","- Here’s how **softmax regression** (a.k.a. **multinomial logistic regression**) differs from standard (binary) **logistic regression**:\n","\n","---\n","\n","## 1. Task / Output\n","\n","* **Logistic Regression**\n","\n","  * **Binary** classification: predicts probability of two classes (e.g. “0” vs. “1”).\n","  * Uses a **sigmoid** function to map a single logit $z = \\beta_0 + \\boldsymbol\\beta^T\\mathbf{x}$ into $(0,1)$:\n","\n","    $$\n","      P(y=1\\mid\\mathbf{x})\n","      = \\frac{1}{1 + e^{-z}}.\n","    $$\n","\n","* **Softmax Regression**\n","\n","  * **Multiclass** classification: predicts probabilities over $K$ mutually exclusive classes.\n","  * Uses the **softmax** function to map a vector of $K$ logits $\\{z_k\\}$ into a probability simplex (they sum to 1):\n","\n","    $$\n","      P(y = k \\mid \\mathbf{x})\n","      = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)},\n","      \\quad\n","      z_k = \\beta_{0}^{(k)} + {\\boldsymbol\\beta^{(k)}}^T\\mathbf{x}.\n","    $$\n","\n","---\n","\n","## 2. Model Formulation\n","\n","| Aspect            | Logistic Regression            | Softmax Regression                                     |\n","| ----------------- | ------------------------------ | ------------------------------------------------------ |\n","| **Logit(s)**      | Single: $z$                    | Vector: $\\mathbf{z} = (z_1,\\dots,z_K)$                 |\n","| **Link function** | Sigmoid $\\sigma(z)$            | Softmax $\\displaystyle \\frac{e^{z_k}}{\\sum_j e^{z_j}}$ |\n","| **Output**        | $P(y=1)$ and $P(y=0)=1-P(y=1)$ | $P(y=k)$ for $k=1,\\dots,K$, $\\sum_kP(y=k)=1$           |\n","\n","---\n","\n","## 3. Loss Function\n","\n","* **Binary Cross-Entropy** (for logistic regression):\n","\n","  $$\n","    \\mathcal{L}\n","    = -\\frac1N\\sum_{i=1}^N \\Bigl[y_i\\log \\hat p_i + (1-y_i)\\log(1-\\hat p_i)\\Bigr].\n","  $$\n","\n","* **Categorical Cross-Entropy** (for softmax regression):\n","\n","  $$\n","    \\mathcal{L}\n","    = -\\frac1N\\sum_{i=1}^N \\sum_{k=1}^K\n","       \\mathbb{1}\\{y_i=k\\}\\,\\log P(y_i=k\\mid \\mathbf{x}_i).\n","  $$\n","\n","---\n","\n","## 4. Training & Complexity\n","\n","* **Logistic Regression**\n","\n","  * Solves a single convex optimization over $(\\beta_0,\\boldsymbol\\beta)$.\n","  * Very efficient—closed-form for ridge, gradient methods for general penalties.\n","\n","* **Softmax Regression**\n","\n","  * Jointly optimizes $K$ weight vectors $\\{\\beta_0^{(k)},\\boldsymbol\\beta^{(k)}\\}$ in one convex problem.\n","  * Slightly higher computational and memory cost (scales with $K$) but still convex and efficiently solvable.\n","\n","---\n","\n","## 5. Relationship Between Them\n","\n","* **Softmax** is the natural generalization of the sigmoid-based binary logistic to more than two classes.\n","* If you apply softmax to $K=2$ logits $(z_1,z_2)$, you recover the same decision boundary as binary logistic on the difference $z_1 - z_2$.\n","\n","---\n","\n","## 6. When to Use Which\n","\n","* **Binary-only problem** → use **logistic regression**.\n","* **Multiclass problem** → use **softmax regression** (or binary logistic with One-vs-Rest, though softmax gives a single, consistent probabilistic model).\n","\n","---\n","\n","**In short**, logistic regression is a special case of softmax regression for $K=2$. Softmax extends the same linear-plus-probability-link idea to handle any number of classes in one unified model.\n","\n","----"],"metadata":{"id":"HN0PU3XvERhL"}},{"cell_type":"markdown","source":["19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n","- When deciding between One-vs-Rest (OvR) and Softmax (multinomial logistic) for a $K$-class problem, weigh these factors:\n","\n","---\n","\n","## 1. Model Consistency & Probabilities\n","\n","* **Softmax**\n","\n","  * Produces a single probability distribution over all $K$ classes ($\\sum_k P_k = 1$).\n","  * Better calibrated—directly optimizes the joint categorical cross-entropy.\n","* **OvR**\n","\n","  * Trains $K$ independent binary classifiers; each gives $P_k = P(y=k)$ but they needn’t sum to 1.\n","  * You typically normalize or simply pick $\\arg\\max P_k$, but probabilities can be inconsistent.\n","\n","**Choose Softmax** if you care about well‐calibrated, comparable probabilities across all classes.\n","\n","---\n","\n","## 2. Computational Cost & Complexity\n","\n","* **Softmax**\n","\n","  * One joint optimization over $K$ weight vectors.\n","  * Cost per iteration scales roughly as $O(n\\,K\\,p)$.\n","* **OvR**\n","\n","  * $K$ separate binary optimizations, each $O(n\\,p)$.\n","  * Total cost $O(n\\,p\\,K)$, but you can parallelize the $K$ fits.\n","\n","**Choose OvR** if $K$ is large and you want to distribute training across multiple workers, or if your solver doesn’t support multinomial loss.\n","\n","---\n","\n","## 3. Class Imbalance & Data Size\n","\n","* **OvR**\n","\n","  * Each binary problem may be highly imbalanced (1 “positive” vs. $(K\\!-\\!1)$ “negatives”).\n","  * You’ll need to tune class weights or resample differently for each classifier.\n","* **Softmax**\n","\n","  * Handles multiclass sample proportions inherently in one loss.\n","\n","**Choose Softmax** when classes are uneven and you’d rather not juggle $K$ different imbalance corrections.\n","\n","---\n","\n","## 4. Interpretability & Debugging\n","\n","* **OvR**\n","\n","  * Easier to inspect each binary decision boundary (“Why is class 3 vs. rest failing?”).\n","  * Good for quick diagnostics or when you only care about a subset of classes.\n","* **Softmax**\n","\n","  * You get a holistic view: every decision boundary is jointly influenced by all classes.\n","  * Slightly harder to disentangle individual effects.\n","\n","**Choose OvR** if per-class interpretability and modular debugging are priorities.\n","\n","---\n","\n","## 5. Solver Support & Extensions\n","\n","* **Softmax**\n","\n","  * Requires a solver that supports the multinomial loss (e.g. `lbfgs`, `newton-cg`, or `saga` in scikit-learn).\n","* **OvR**\n","\n","  * Works with any binary solver (including those that only support L1 or specialized regularizers).\n","\n","**Choose OvR** if you need a particular penalty or solver that doesn’t support multinomial logistic.\n","\n","---\n","\n","## Decision-Rule of Thumb\n","\n","| Criterion                                       | Prefer OvR        | Prefer Softmax |\n","| ----------------------------------------------- | ----------------- | -------------- |\n","| $K$ very large                                  | ✔️ parallelizable |                |\n","| Need per-class binary diagnostics               | ✔️                |                |\n","| Solver only supports binary loss                | ✔️                |                |\n","| Need calibrated probabilities                   |                   | ✔️             |\n","| Classes highly imbalanced                       |                   | ✔️             |\n","| Want the statistically “optimal” multiclass fit |                   | ✔️             |\n","\n","---\n","\n","**In practice**:\n","\n","* If you’re using a modern library with multinomial support and want the cleanest probabilities, start with **Softmax** (`multi_class='multinomial'`).\n","* If you hit solver limitations, need extreme parallelism, or want per-class control, switch to **OvR** (`multi_class='ovr'`).\n","\n","---"],"metadata":{"id":"f6t-iPPvEgJ5"}},{"cell_type":"markdown","source":["20.  How do we interpret coefficients in Logistic Regression?\n","- In logistic regression, each coefficient $\\beta_j$ quantifies how its associated predictor $x_j$ shifts the **log-odds** of the outcome. Here’s how to read them:\n","\n","---\n","\n","## 1. Log-Odds Interpretation\n","\n","The core model is\n","\n","$$\n","\\ln\\!\\frac{P(y=1\\mid \\mathbf{x})}{P(y=0\\mid \\mathbf{x})}\n","= \\beta_0 \\;+\\;\\sum_{j=1}^p \\beta_j\\,x_j.\n","$$\n","\n","* **$\\beta_j$** is the change in **log-odds** of “success” ($y=1$) for a **one-unit increase** in $x_j$, **holding all other $x$’s constant**.\n","\n","  * If $\\beta_j = 0.7$, then increasing $x_j$ by 1 raises the log-odds by 0.7.\n","\n","---\n","\n","## 2. Odds Ratio Interpretation\n","\n","Exponentiating turns log-odds into **odds**:\n","\n","$$\n","\\text{odds ratio for }x_j\n","= e^{\\beta_j}.\n","$$\n","\n","* **$e^{\\beta_j}$** is the factor by which the **odds** multiply when $x_j$ increases by one unit.\n","\n","  * If $\\beta_j = 0.7$, then $e^{0.7}\\approx2.01$: the odds of $y=1$ are about **twice** as large for each one-unit jump in $x_j$.\n","\n","---\n","\n","## 3. Examples\n","\n","1. **Continuous Predictor**\n","\n","   * $x_1 =$ “years of experience,” $\\beta_1 = 0.2$.\n","   * Interpretation: Each additional year multiplies the odds of success by $e^{0.2}\\approx1.22$ (a 22% increase in odds).\n","\n","2. **Binary (0/1) Predictor**\n","\n","   * $x_2 =$ “has insurance” (0 = no, 1 = yes), $\\beta_2 = -1.1$.\n","   * Interpretation: Having insurance multiplies the odds of the event by $e^{-1.1}\\approx0.33$, i.e. it **reduces** the odds to one-third.\n","\n","---\n","\n","## 4. Intercept ($\\beta_0$)\n","\n","* $\\beta_0$ is the log-odds of $y=1$ when **all** $x_j = 0$.\n","* The **baseline odds** are $e^{\\beta_0}$, and the **baseline probability** is\n","\n","  $$\n","    \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}.\n","  $$\n","\n","---\n","\n","## 5. From Odds to Probability Change\n","\n","Because the link is non-linear, a unit change in $x_j$ doesn’t add a fixed amount to the **probability** $P(y=1)$; it adds a fixed amount to the **log-odds**. The marginal change in probability depends on the current location on the sigmoid curve:\n","\n","$$\n","\\Delta P \\approx \\beta_j \\times \\sigma(z)\\bigl(1-\\sigma(z)\\bigr),\n","$$\n","\n","where $z = \\beta_0 + \\sum \\beta_j x_j$. In practice, you interpret probabilities at meaningful “anchor” points (e.g., median $x$).\n","\n","---\n","\n","## 6. Standardization for Comparability\n","\n","* If predictors are on very different scales, it’s common to **standardize** ($z$-score) them.\n","* Then each $\\beta_j$ reflects the change in log-odds per **one standard-deviation** increase in $x_j$, making coefficients directly comparable in magnitude.\n","\n","---\n","\n","### In a Nutshell\n","\n","1. **$\\beta_j$** = change in **log-odds** per unit $x_j$.\n","2. **$e^{\\beta_j}$** = **odds ratio** per unit $x_j$.\n","3. **Probability effects** vary with the baseline—use predicted probabilities at reference values to illustrate real-world impact.\n","\n","---"],"metadata":{"id":"ijj1kUw3E2R3"}},{"cell_type":"markdown","source":["#Practical"],"metadata":{"id":"lOtuG0m3GjVu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdhbrBme_B-Y","executionInfo":{"status":"ok","timestamp":1750603640311,"user_tz":-330,"elapsed":2776,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"74e6a2e5-88d1-44ca-c719-5d8d798f77af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Accuracy: 100.0 %\n"]}],"source":["'''\n","1.  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n","Regression, and prints the model accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load a sample dataset (Iris)\n","data = load_iris()\n","df = pd.DataFrame(data=data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# To make it a binary classification (for logistic regression), filter two classes only\n","df = df[df['target'] != 2]  # Keep only class 0 and 1\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train logistic regression model\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Logistic Regression Accuracy:\", round(accuracy * 100, 2), \"%\")\n"]},{"cell_type":"code","source":["'''\n","2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n","and print the model accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load a sample dataset (Iris)\n","data = load_iris()\n","df = pd.DataFrame(data=data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# To make it a binary classification (for logistic regression), filter two classes only\n","df = df[df['target'] != 2]  # Keep only class 0 and 1\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train logistic regression model with L1 regularization\n","model = LogisticRegression(penalty='l1', solver='liblinear')  # liblinear supports L1\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"L1-Regularized Logistic Regression Accuracy:\", round(accuracy * 100, 2), \"%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Lx1xZnwG3PI","executionInfo":{"status":"ok","timestamp":1750603763202,"user_tz":-330,"elapsed":21,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"c75ff544-8d1e-4caa-b3f7-04846a06673d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["L1-Regularized Logistic Regression Accuracy: 100.0 %\n"]}]},{"cell_type":"code","source":["'''\n","3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n","LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the dataset (Iris)\n","data = load_iris()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Filter for binary classification (only classes 0 and 1)\n","df = df[df['target'] != 2]\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train logistic regression model with L2 regularization (Ridge)\n","model = LogisticRegression(penalty='l2', solver='liblinear')\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"L2-Regularized Logistic Regression Accuracy:\", round(accuracy * 100, 2), \"%\")\n","\n","# Print coefficients\n","print(\"Model Coefficients:\")\n","for feature, coef in zip(X.columns, model.coef_[0]):\n","    print(f\"{feature}: {coef:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MAhbkWfHYuU","executionInfo":{"status":"ok","timestamp":1750604013019,"user_tz":-330,"elapsed":72,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"f3cb08fe-7a14-4177-86c3-fa69c3c02da0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["L2-Regularized Logistic Regression Accuracy: 100.0 %\n","Model Coefficients:\n","sepal length (cm): -0.3754\n","sepal width (cm): -1.3966\n","petal length (cm): 2.1525\n","petal width (cm): 0.9642\n"]}]},{"cell_type":"code","source":["'''\n","4.  Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the dataset (Iris)\n","data = load_iris()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Filter for binary classification (only classes 0 and 1)\n","df = df[df['target'] != 2]\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train logistic regression model with Elastic Net regularization\n","model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Elastic Net Regularized Logistic Regression Accuracy:\", round(accuracy * 100, 2), \"%\")\n","\n","# Print coefficients\n","print(\"Model Coefficients:\")\n","for feature, coef in zip(X.columns, model.coef_[0]):\n","    print(f\"{feature}: {coef:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rdO1QBWIVuZ","executionInfo":{"status":"ok","timestamp":1750604193385,"user_tz":-330,"elapsed":21,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"8d6a64ac-e31d-4404-c59c-fecf80aba279"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Elastic Net Regularized Logistic Regression Accuracy: 100.0 %\n","Model Coefficients:\n","sepal length (cm): 0.0000\n","sepal width (cm): -0.7319\n","petal length (cm): 2.6305\n","petal width (cm): 0.5772\n"]}]},{"cell_type":"code","source":["'''\n","5.  Write a Python program to train a Logistic Regression model for multiclass classification using\n","multi_class='ovr\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the full Iris dataset (multiclass)\n","data = load_iris()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train logistic regression model with One-vs-Rest strategy\n","model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n","model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = model.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Multiclass Logistic Regression (OvR) Accuracy:\", round(accuracy * 100, 2), \"%\")\n","\n","# Print coefficients per class\n","print(\"\\nModel Coefficients:\")\n","for i, class_label in enumerate(model.classes_):\n","    print(f\"Class {class_label} Coefficients:\")\n","    for feature, coef in zip(X.columns, model.coef_[i]):\n","        print(f\"  {feature}: {coef:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxuMC-LJJBpz","executionInfo":{"status":"ok","timestamp":1750604275492,"user_tz":-330,"elapsed":126,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"5df43e87-d3e5-4b44-f175-5a855f077f11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Multiclass Logistic Regression (OvR) Accuracy: 100.0 %\n","\n","Model Coefficients:\n","Class 0 Coefficients:\n","  sepal length (cm): 0.3711\n","  sepal width (cm): 1.4097\n","  petal length (cm): -2.1521\n","  petal width (cm): -0.9547\n","Class 1 Coefficients:\n","  sepal length (cm): 0.4940\n","  sepal width (cm): -1.5890\n","  petal length (cm): 0.4372\n","  petal width (cm): -1.1119\n","Class 2 Coefficients:\n","  sepal length (cm): -1.5590\n","  sepal width (cm): -1.5889\n","  petal length (cm): 2.3987\n","  petal width (cm): 2.1556\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["'''\n","6.  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n","Regression. Print the best parameters and accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load the full Iris dataset (multiclass)\n","data = load_iris()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the parameter grid for GridSearchCV\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10, 100],\n","    'penalty': ['l1', 'l2'],\n","    'solver': ['liblinear']  # liblinear supports both l1 and l2\n","}\n","\n","# Initialize GridSearchCV with Logistic Regression\n","grid_search = GridSearchCV(LogisticRegression(multi_class='ovr', max_iter=1000), param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","\n","# Predict on test set with best estimator\n","y_pred = grid_search.best_estimator_.predict(X_test)\n","\n","# Calculate and print accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best Model Accuracy:\", round(accuracy * 100, 2), \"%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qhbzWbQJVyP","executionInfo":{"status":"ok","timestamp":1750604397415,"user_tz":-330,"elapsed":1470,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"58b9e976-bbc3-4847-b146-45e015e0a755"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n","Best Model Accuracy: 100.0 %\n"]}]},{"cell_type":"code","source":["'''\n","7.  Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n","average accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","import numpy as np\n","\n","# Load the full Iris dataset (multiclass)\n","data = load_iris()\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Define features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Initialize Logistic Regression model\n","model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n","\n","# Set up Stratified K-Fold Cross-Validation\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Perform cross-validation and calculate accuracy\n","accuracies = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n","\n","# Print each fold's accuracy and the average accuracy\n","print(\"Stratified K-Fold Accuracies:\", np.round(accuracies * 100, 2))\n","print(\"Average Accuracy:\", round(np.mean(accuracies) * 100, 2), \"%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OeL3N9HiJzO4","executionInfo":{"status":"ok","timestamp":1750604503382,"user_tz":-330,"elapsed":30,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"5441b0be-2a98-4d1e-feaf-fe03d318f1cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stratified K-Fold Accuracies: [ 96.67 100.    90.    93.33 100.  ]\n","Average Accuracy: 96.0 %\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["'''\n","8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n","accuracy.\n","'''\n","from sklearn.datasets import load_iris\n","import pandas as pd\n","\n","# Load Iris dataset\n","iris = load_iris(as_frame=True)\n","df = iris.frame\n","\n","# Save to CSV\n","df.to_csv(\"iris.csv\", index=False)\n","print(\"Saved sample dataset as iris.csv\")\n","\n","# Load dataset\n","df = pd.read_csv(\"iris.csv\")\n","\n","# For binary classification, use only two classes (e.g., target < 2)\n","df = df[df['target'] < 2]\n","\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Rest of the Logistic Regression code\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load the dataset\n","# Replace 'data.csv' with your actual CSV file path\n","data = pd.read_csv('iris.csv')\n","\n","# Replace 'target' with the actual target column name\n","y = data['target']\n","X = data.drop('target', axis=1)\n","\n","# Split into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train the Logistic Regression model\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n"],"metadata":{"id":"wglywKPOKNR_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750861192310,"user_tz":-330,"elapsed":78,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"a471fc23-cd09-4efa-9b84-90edc83ebd15"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved sample dataset as iris.csv\n","Accuracy: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n","Logistic Regression. Print the best parameters and accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from scipy.stats import uniform\n","\n","# Load your dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the model\n","model = LogisticRegression(max_iter=1000)\n","\n","# Define hyperparameter space\n","param_distributions = {\n","    'C': uniform(loc=0.01, scale=10),\n","    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n","    'solver': ['saga', 'liblinear', 'lbfgs', 'newton-cg', 'sag']\n","}\n","\n","# Setup RandomizedSearchCV\n","random_search = RandomizedSearchCV(\n","    estimator=model,\n","    param_distributions=param_distributions,\n","    n_iter=50,\n","    cv=5,\n","    scoring='accuracy',\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","# Fit the model\n","random_search.fit(X_train, y_train)\n","\n","# Best parameters\n","print(\"Best Parameters:\", random_search.best_params_)\n","\n","# Evaluate on test set\n","y_pred = random_search.best_estimator_.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy on test data: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FA1VwJoSb51V","executionInfo":{"status":"ok","timestamp":1750861317652,"user_tz":-330,"elapsed":6525,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"4d5478b3-6473-4c48-8196-e8e7bd1eb2ec"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'C': np.float64(2.9727350570408237), 'penalty': 'l1', 'solver': 'saga'}\n","Accuracy on test data: 1.00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n","170 fits failed out of a total of 250.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","15 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","15 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n","\n","--------------------------------------------------------------------------------\n","60 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n","    estimator._validate_params()\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n","\n","--------------------------------------------------------------------------------\n","20 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n","    estimator._validate_params()\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n","\n","--------------------------------------------------------------------------------\n","10 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n","    raise ValueError(\n","ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n","\n","--------------------------------------------------------------------------------\n","20 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n","    raise ValueError(\n","ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n","\n","--------------------------------------------------------------------------------\n","15 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n","    raise ValueError(\n","ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n","\n","--------------------------------------------------------------------------------\n","10 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n","    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n","ValueError: l1_ratio must be specified when penalty is elasticnet.\n","\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n","    raise ValueError(\n","ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.95833333        nan        nan 0.95       0.94166667\n","        nan        nan        nan        nan        nan        nan\n"," 0.95       0.95833333 0.95833333        nan        nan        nan\n"," 0.95833333 0.95833333 0.95833333 0.94166667        nan 0.95833333\n","        nan 0.95833333 0.96666667 0.93333333        nan        nan\n","        nan 0.96666667        nan        nan        nan        nan\n","        nan        nan        nan        nan        nan        nan\n","        nan        nan        nan        nan 0.96666667        nan\n","        nan        nan]\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["'''\n","10.  Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multiclass import OneVsOneClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load your dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the base logistic regression model\n","base_model = LogisticRegression(max_iter=1000)\n","\n","# Wrap the model using One-vs-One strategy\n","ovo_model = OneVsOneClassifier(base_model)\n","\n","# Train the model\n","ovo_model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = ovo_model.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"One-vs-One Multiclass Logistic Regression Accuracy: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CShOUzeIdv4s","executionInfo":{"status":"ok","timestamp":1750862446184,"user_tz":-330,"elapsed":425,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"6c77c806-71f3-4f52-b4f5-2c79cecdc70e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["One-vs-One Multiclass Logistic Regression Accuracy: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n","classification\n","'''\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","# Load dataset\n","data = pd.read_csv('iris.csv')\n","\n","# Drop missing values\n","data.dropna(inplace=True)\n","\n","# Automatically convert to binary by selecting the first two unique classes\n","unique_classes = data['target'].unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","\n","# Define X and y\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train model\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = model.predict(X_test)\n","\n","# Confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Plotting confusion matrix\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix - Logistic Regression')\n","plt.show()\n","\n","# Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"HKMwwYn8gRQV","executionInfo":{"status":"ok","timestamp":1750862954253,"user_tz":-330,"elapsed":358,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"3ee1ef47-d0fd-43ed-bbe4-f76ad7c3e0a3"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Accuracy: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","12.  Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n","Recall, and F1-Score\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Load your dataset\n","# Replace 'data.csv' and 'target' with actual values\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train the Logistic Regression model\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate performance\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(f\"Precision: {precision:.2f}\")\n","print(f\"Recall: {recall:.2f}\")\n","print(f\"F1-Score: {f1:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhOhoaD_iwmZ","executionInfo":{"status":"ok","timestamp":1750863056609,"user_tz":-330,"elapsed":117,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"06868ec6-354b-4748-fd54-cb58061a9a0f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 1.00\n","Recall: 1.00\n","F1-Score: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n","improve model performance\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with actual values\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Split dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Train Logistic Regression with class weights\n","model = LogisticRegression(class_weight='balanced', max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","\n","# Evaluation\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_test, y_pred)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.title('Confusion Matrix with Class Weights')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"FdELG984kgx5","executionInfo":{"status":"ok","timestamp":1750863153101,"user_tz":-330,"elapsed":347,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"39e461af-2acb-484a-bc4f-4880f1960fa1"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       1.00      1.00      1.00        10\n","\n","    accuracy                           1.00        20\n","   macro avg       1.00      1.00      1.00        20\n","weighted avg       1.00      1.00      1.00        20\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["'''\n","14.  Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n","evaluate performance\n","'''\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Load Titanic dataset from seaborn\n","raw_data = sns.load_dataset('titanic')\n","data = raw_data.copy()\n","\n","# Drop columns with too many missing values or irrelevant for prediction\n","data.drop(['deck', 'embark_town', 'alive', 'class', 'who'], axis=1, inplace=True)\n","\n","# Drop rows with missing target ('survived') or features like 'age' and 'embarked'\n","data.dropna(subset=['survived', 'age', 'embarked'], inplace=True)\n","\n","# Fill missing 'embarked' with mode if any remain\n","data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n","\n","# Encode categorical features\n","label_encoders = {}\n","for col in ['sex', 'embarked', 'alone']:\n","    le = LabelEncoder()\n","    data[col] = le.fit_transform(data[col])\n","    label_encoders[col] = le\n","\n","# Define features and target\n","X = data.drop('survived', axis=1)\n","y = data['survived']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = model.predict(X_test)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6HPc3RVbk4Qn","executionInfo":{"status":"ok","timestamp":1750863250276,"user_tz":-330,"elapsed":443,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"2ce4a832-30da-4838-a8ed-53c21f37c83a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-19-522915814.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7902097902097902\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.79      0.85      0.82        80\n","           1       0.79      0.71      0.75        63\n","\n","    accuracy                           0.79       143\n","   macro avg       0.79      0.78      0.78       143\n","weighted avg       0.79      0.79      0.79       143\n","\n"]}]},{"cell_type":"code","source":["'''\n","15.  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n","model. Evaluate its accuracy and compare results with and without scaling\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with actual values\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Logistic Regression without scaling\n","model_no_scaling = LogisticRegression(max_iter=1000)\n","model_no_scaling.fit(X_train, y_train)\n","y_pred_no_scaling = model_no_scaling.predict(X_test)\n","accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n","\n","# Apply Standardization\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Logistic Regression with scaling\n","model_scaled = LogisticRegression(max_iter=1000)\n","model_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = model_scaled.predict(X_test_scaled)\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","# Compare results\n","print(f\"Accuracy without scaling: {accuracy_no_scaling:.2f}\")\n","print(f\"Accuracy with standardization: {accuracy_scaled:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqqjIPPilP-3","executionInfo":{"status":"ok","timestamp":1750863318159,"user_tz":-330,"elapsed":85,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"de3e7478-02d0-4849-9229-46e2d76c96cb"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy without scaling: 1.00\n","Accuracy with standardization: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with actual values\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression model\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict probabilities\n","y_probs = model.predict_proba(X_test)[:, 1]\n","\n","# Calculate ROC-AUC score\n","roc_auc = roc_auc_score(y_test, y_probs)\n","print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n","\n","# Plot ROC Curve\n","fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n","plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve - Logistic Regression')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"id":"2576fCPQlgoy","executionInfo":{"status":"ok","timestamp":1750863400216,"user_tz":-330,"elapsed":873,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"ad4ed7c9-3921-4b34-cd7a-cd92f382b37f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["ROC-AUC Score: 1.00\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["'''\n","17.  Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n","accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with actual file and column name\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression with custom learning rate (C=0.5)\n","model = LogisticRegression(C=0.5, max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy with C=0.5: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvxk8GXfl0er","executionInfo":{"status":"ok","timestamp":1750863486715,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"c83be350-8219-4cc4-9d3f-c7ae72860828"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with C=0.5: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","18. Write a Python program to train Logistic Regression and identify important features based on model\n","coefficients\n","'''\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Get feature importance (coefficients)\n","coefficients = model.coef_[0]\n","feature_names = X.columns\n","feature_importance = pd.Series(coefficients, index=feature_names).sort_values(key=abs, ascending=False)\n","\n","# Display important features\n","print(\"Important Features Based on Coefficients:\")\n","print(feature_importance)\n","\n","# Visualize feature importance\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x=feature_importance.values, y=feature_importance.index, palette='coolwarm')\n","plt.title('Feature Importance (Logistic Regression Coefficients)')\n","plt.xlabel('Coefficient Value')\n","plt.ylabel('Feature')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":818},"id":"a9nwqOwlmJy8","executionInfo":{"status":"ok","timestamp":1750863563435,"user_tz":-330,"elapsed":857,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"44e9da42-bfc8-40fd-d875-c01a5682ee3b"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Important Features Based on Coefficients:\n","petal length (cm)    2.212332\n","petal width (cm)     0.927443\n","sepal width (cm)    -0.844458\n","sepal length (cm)    0.462485\n","dtype: float64\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-23-3683700034.py:43: FutureWarning: \n","\n","Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n","\n","  sns.barplot(x=feature_importance.values, y=feature_importance.index, palette='coolwarm')\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbXhJREFUeJzt3XdclfX///HnAVkyRUFEUXCBA/deYKmk5Sczs9Rya0NLc2WZg/o4y9zasETNMjO1PmmmqbhzZYY7ETVLcw/EgXD9/vDH+XoEFJBLFB73282bnOt6X9f7da73OQee51oWwzAMAQAAAACAbGeX0wUAAAAAAJBbEboBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAQCp//fWXnJ2dtXHjxpwuRRaLRSNGjMiWdR05ckQWi0VRUVHZsj5I0dHRslgsio6OzulSHribN29q0KBBCggIkJ2dnVq1aiVJio+PV/fu3eXn5yeLxaK+fftm+bUXFRUli8WiI0eOZHv92aVOnToaNGhQTpcBPLQI3QDwgKX8AZXWv8GDB5vS56ZNmzRixAhduHDBlPXfj5TtsX379pwuJcumT5+e60Lce++9p9q1a6t+/frWaZ07d5abm1sOVpVxX331lSZOnGhqHykhKuWfnZ2dvL291bx5c23evNnUvmFr8eLFat68uQoVKiRHR0f5+/urbdu2Wr16tan9fvHFF/rggw/Upk0bzZ49W2+++aYkadSoUYqKitKrr76quXPn6qWXXjK1jvuVkJCgESNGZPmLk7feekvTpk3TyZMns7cwIJfIl9MFAEBe9d577ykoKMhmWsWKFU3pa9OmTYqMjFTnzp3l5eVlSh952fTp01WoUCF17tw5p0vJFqdPn9bs2bM1e/bsnC5FknT16lXly5e5P1m++uor7d69W3379rWZXqJECV29elUODg7ZVl+7du3UokULJSUl6eDBg5o+fboaN26sbdu2KTQ0NNv6eVg1atRIV69elaOj4wPv2zAMde3aVVFRUapatar69esnPz8/nThxQosXL9bjjz+ujRs3ql69eqb0v3r1ahUtWlQTJkxINb1OnToaPny4Ta1Zee299NJLeuGFF+Tk5JQtNaclISFBkZGRkqTw8PBML//000/Lw8ND06dP13vvvZfN1QGPPkI3AOSQ5s2bq0aNGjldxn25cuWKXF1dc7qMHJOQkKD8+fPndBnZ7ssvv1S+fPnUsmXLnC5FkuTs7Jxt67JYLNm6PkmqVq2aXnzxRevjhg0bqnnz5poxY4amT5+erX3dS068J+3s7LJ9m2bU+PHjFRUVpb59++qjjz6SxWKxzhsyZIjmzp2b6S9sMuPUqVNpfpF56tQplS9f3mZaVl979vb2sre3z2qJD4SdnZ3atGmjOXPmKDIy0mYcAHB4OQA8tH766Sc1bNhQrq6ucnd315NPPqk9e/bYtPnjjz/UuXNnlSxZUs7OzvLz81PXrl119uxZa5sRI0Zo4MCBkqSgoCDrobBHjhy56zmGd55HO2LECFksFu3du1ft27dXgQIF1KBBA+v8L7/8UtWrV5eLi4u8vb31wgsv6K+//srSc085jPnYsWN66qmn5ObmpqJFi2ratGmSpJiYGD322GNydXVViRIl9NVXX9ksn3LI+rp16/Tyyy+rYMGC8vDwUMeOHXX+/PlU/U2fPl0VKlSQk5OT/P391atXr1SH4oeHh6tixYrasWOHGjVqpPz58+udd95RYGCg9uzZo7Vr11q3bcqeonPnzmnAgAEKDQ2Vm5ubPDw81Lx5c+3atctm3SnnxC5YsEAjR45UsWLF5OzsrMcff1yHDh1KVe+WLVvUokULFShQQK6urqpUqZImTZpk02b//v1q06aNvL295ezsrBo1auiHH37I0PZfsmSJateuneVDyb/99lvra6FQoUJ68cUX9ffff6fZrnz58nJ2dlbFihW1ePFide7cWYGBgTbt7nwtXr58WX379lVgYKCcnJzk6+urpk2b6rfffpN0a6yWLl2qo0ePWsckZZ3pveb379+vtm3bysfHRy4uLgoODtaQIUOy9PwbNmwoSYqNjbWZfuHCBfXt21cBAQFycnJS6dKlNXbsWCUnJ9u0O3v2rF566SV5eHjIy8tLnTp10q5du1LVnfI+iY2NVYsWLeTu7q4OHTpIkpKTkzVx4kRVqFBBzs7OKly4sF5++eVUr//t27crIiJChQoVkouLi4KCgtS1a1ebNvPnz1f16tXl7u4uDw8PhYaG2rze0junOyOvg5Tn8Pfff6tVq1Zyc3OTj4+PBgwYoKSkpLtu56tXr2r06NEKCQnRhx9+mGbQe+mll1SrVi3r48OHD+u5556Tt7e38ufPrzp16mjp0qWplrt+/bqGDx+u0qVLy8nJSQEBARo0aJCuX78u6f9eR2vWrNGePXusr7OUbREXF6elS5dm6PP2Xq+99M7pzsjviIxs3yNHjsjHx0eSrIH59vfcyZMn1aVLFxUrVkxOTk4qUqSInn766VT1NG3aVEePHtXvv/+e7pgBeRV7ugEgh1y8eFFnzpyxmVaoUCFJ0ty5c9WpUydFRERo7NixSkhI0IwZM9SgQQPt3LnTGiBWrlypw4cPq0uXLvLz89OePXv06aefas+ePfr1119lsVjUunVrHTx4UF9//bUmTJhg7cPHx0enT5/OdN3PPfecypQpo1GjRskwDEnSyJEjNXToULVt21bdu3fX6dOnNWXKFDVq1Eg7d+7M0iHtSUlJat68uRo1aqRx48Zp3rx56t27t1xdXTVkyBB16NBBrVu31scff6yOHTuqbt26qQ7X7927t7y8vDRixAgdOHBAM2bM0NGjR61/GEu3vkyIjIxUkyZN9Oqrr1rbbdu2TRs3brQ5FPTs2bNq3ry5XnjhBb344osqXLiwwsPD9frrr8vNzc36h3LhwoUl3foDf8mSJXruuecUFBSkf//9V5988onCwsK0d+9e+fv729Q7ZswY2dnZacCAAbp48aLGjRunDh06aMuWLdY2K1eu1FNPPaUiRYqoT58+8vPz0759+/Tjjz+qT58+kqQ9e/aofv36Klq0qAYPHixXV1ctWLBArVq10nfffadnnnkm3e2emJiobdu26dVXX830mEm3AkKXLl1Us2ZNjR49Wv/++68mTZqkjRs32rwWli5dqueff16hoaEaPXq0zp8/r27duqlo0aL37OOVV17RwoUL1bt3b5UvX15nz57Vhg0btG/fPlWrVk1DhgzRxYsXdfz4cethv3f7AuGPP/5Qw4YN5eDgoJ49eyowMFCxsbH63//+p5EjR2Z6G6SEkQIFClinJSQkKCwsTH///bdefvllFS9eXJs2bdLbb7+tEydOWM8/T05OVsuWLbV161a9+uqrCgkJ0ffff69OnTql2dfNmzcVERGhBg0a6MMPP7QeefHyyy9bx+KNN95QXFycpk6dqp07d1pf16dOnVKzZs3k4+OjwYMHy8vLS0eOHNGiRYus61+5cqXatWunxx9/XGPHjpUk7du3Txs3brS+3tKS0deBdOu9HhERodq1a+vDDz/UL7/8ovHjx6tUqVJ3fR1u2LBB586dU9++fTO0J/jff/9VvXr1lJCQoDfeeEMFCxbU7Nmz9Z///EcLFy60vi+Sk5P1n//8Rxs2bFDPnj1Vrlw5xcTEaMKECTp48KCWLFkiHx8fzZ07VyNHjlR8fLxGjx4tSSpXrpzmzp2rN998U8WKFVP//v0lpf95m9XXXkZ/R2Rk+/r4+GjGjBl69dVX9cwzz6h169aSpEqVKkmSnn32We3Zs0evv/66AgMDderUKa1cuVLHjh2z6ad69eqSpI0bN6pq1ar3HA8gTzEAAA/UrFmzDElp/jMMw7h8+bLh5eVl9OjRw2a5kydPGp6enjbTExISUq3/66+/NiQZ69ats0774IMPDElGXFycTdu4uDhDkjFr1qxU65FkDB8+3Pp4+PDhhiSjXbt2Nu2OHDli2NvbGyNHjrSZHhMTY+TLly/V9PS2x7Zt26zTOnXqZEgyRo0aZZ12/vx5w8XFxbBYLMb8+fOt0/fv35+q1pR1Vq9e3bhx44Z1+rhx4wxJxvfff28YhmGcOnXKcHR0NJo1a2YkJSVZ202dOtWQZHzxxRfWaWFhYYYk4+OPP071HCpUqGCEhYWlmn7t2jWb9RrGrW3u5ORkvPfee9Zpa9asMSQZ5cqVM65fv26dPmnSJEOSERMTYxiGYdy8edMICgoySpQoYZw/f95mvcnJydafH3/8cSM0NNS4du2azfx69eoZZcqUSVXn7Q4dOmRIMqZMmZJqXqdOnQxXV9d0l71x44bh6+trVKxY0bh69ap1+o8//mhIMoYNG2adFhoaahQrVsy4fPmydVp0dLQhyShRooTNeu8cX09PT6NXr153fR5PPvlkqvUYRtqv+UaNGhnu7u7G0aNHbdrevk3TkrKuyMhI4/Tp08bJkyeN9evXGzVr1jQkGd9++6217fvvv2+4uroaBw8etFnH4MGDDXt7e+PYsWOGYRjGd999Z0gyJk6caG2TlJRkPPbYY6nqTnmfDB482Gad69evNyQZ8+bNs5m+fPlym+mLFy9O9d67U58+fQwPDw/j5s2b6bZJef2uWbPGMIzMvQ5SnsPt7wfDMIyqVasa1atXT7dPw/i/98fixYvv2i5F3759DUnG+vXrrdMuX75sBAUFGYGBgdb36ty5cw07OzubdoZhGB9//LEhydi4caN1WlhYmFGhQoVUfZUoUcJ48sknbaZl9bWX8nmW8vmdmd8RGd2+p0+fTvU+M4xbn7uSjA8++CDVc0yLo6Oj8eqrr2aoLZCXcHg5AOSQadOmaeXKlTb/pFt7li5cuKB27drpzJkz1n/29vaqXbu21qxZY12Hi4uL9edr167pzJkzqlOnjiRZD7XNbq+88orN40WLFik5OVlt27a1qdfPz09lypSxqTezunfvbv3Zy8tLwcHBcnV1Vdu2ba3Tg4OD5eXlpcOHD6davmfPnjZ7ql999VXly5dPy5YtkyT98ssvunHjhvr27Ss7u//7ldijRw95eHikOuzUyclJXbp0yXD9Tk5O1vUmJSXp7NmzcnNzU3BwcJrj06VLF5uLUaUcppzy3Hbu3Km4uDj17ds31dEDKXvuz507p9WrV6tt27a6fPmydTzOnj2riIgI/fnnn2ke6p0i5dSE2/fSZtT27dt16tQpvfbaazbnrj755JMKCQmxbs9//vlHMTEx6tixo80e6LCwsAxdeMzLy0tbtmzRP//8k+ka73T69GmtW7dOXbt2VfHixW3mZfS81OHDh8vHx0d+fn5q2LCh9u3bp/Hjx6tNmzbWNt9++60aNmyoAgUK2LxPmjRpoqSkJK1bt06StHz5cjk4OKhHjx7WZe3s7NSrV690+79zb/C3334rT09PNW3a1Kav6tWry83NzfqeTHkN/fjjj0pMTExz3V5eXrpy5Yr18ykjMvo6uN2dnysNGzZM8z19u0uXLkmS3N3dM1TXsmXLVKtWLZvTYtzc3NSzZ08dOXJEe/fulXRr+5UrV04hISE22++xxx6TpPv6TLtdVl97mfkdkSIr21e69TvG0dFR0dHRaZ6ac6eU1zcAWxxeDgA5pFatWmleSO3PP/+UJOsfeHfy8PCw/nzu3DlFRkZq/vz5OnXqlE27ixcvZmO1/+fOQ7j//PNPGYahMmXKpNk+q1eJdnZ2tp5nmMLT01PFihVL9Qepp6dnmn8Q3lmTm5ubihQpYj389+jRo5JuBffbOTo6qmTJktb5KYoWLZqpKzQnJydr0qRJmj59uuLi4mzOUS1YsGCq9nf+4Z0SfFOeW8o5wne7yv2hQ4dkGIaGDh2qoUOHptnm1KlT9zyM2/j/pw5kRnrbU5JCQkK0YcMGm3alS5dO1a506dL3/MJo3Lhx6tSpkwICAlS9enW1aNFCHTt2VMmSJTNdc0rwuJ87B/Ts2VPPPfecrl27ptWrV2vy5Mmpzkf+888/9ccff6R6TadIef8ePXpURYoUSXWBvrS2lSTly5dPxYoVS9XXxYsX5evre9e+wsLC9OyzzyoyMlITJkxQeHi4WrVqpfbt21uvlP3aa69pwYIFat68uYoWLapmzZqpbdu2euKJJ9LdHhl9HaRI671eoECBe4a8lM/Cy5cv37Xd7XXVrl071fRy5cpZ51esWFF//vmn9u3bd8+xul9Zfe1l5neElPXtK9364nDs2LHq37+/ChcurDp16uipp55Sx44d5efnl6q9YRhcRA1IA6EbAB4yKRdVmjt3bpp/1Nx+Jd62bdtq06ZNGjhwoKpUqSI3NzclJyfriSeeSHVxprSk98fR3S5gdPve9ZR6LRaLfvrppzTPq8zqxbjSO0czvelZCYmZdedzv5dRo0Zp6NCh6tq1q95//315e3vLzs5Offv2TXN8suO5pax3wIABioiISLNNegFO+r8vAzLyB3lOadu2rRo2bKjFixdrxYoV+uCDDzR27FgtWrRIzZs3f+D1lClTRk2aNJEkPfXUU7K3t9fgwYPVuHFj6xdrycnJatq0qQYNGpTmOsqWLZulvm8/miJFcnKyfH19NW/evDSXSQlgFotFCxcu1K+//qr//e9/+vnnn9W1a1eNHz9ev/76q9zc3OTr66vff/9dP//8s3766Sf99NNPmjVrljp27Jhtt5TL6pW5Q0JCJN26sGKrVq2ypRbp1vYLDQ3VRx99lOb8gICAbOsrKzLzO0LK+vZN0bdvX7Vs2VJLlizRzz//rKFDh2r06NFavXp1qnO3L1y4YL1uCID/Q+gGgIdMqVKlJEm+vr7WP+TTcv78ea1atUqRkZEaNmyYdXrKXpDbpReuU/ak3nml7jv38N6rXsMwFBQUlOXgYJY///xTjRs3tj6Oj4/XiRMn1KJFC0m37tksSQcOHLDZS3rjxg3FxcXddfvfLr3tu3DhQjVu3Fiff/65zfSs/mGa8trYvXt3urWlPA8HB4cM13+74sWLy8XFRXFxcZle9vbteedeuAMHDljnp/yf1pXZ05qWliJFiui1117Ta6+9plOnTqlatWoaOXKkNXRndG9byvbavXt3htpnxJAhQ/TZZ5/p3Xff1fLlyyXdGrv4+Ph7jkmJEiW0Zs2aVLejy+h2Senrl19+Uf369TP0RVGdOnVUp04djRw5Ul999ZU6dOig+fPnW0/vcHR0VMuWLdWyZUslJyfrtdde0yeffKKhQ4em+QVORl8H96tBgwYqUKCAvv76a73zzjv3DJclSpTQgQMHUk3fv3+/Td2lSpXSrl279Pjjj5u61zarr72M/o7IjHs9z1KlSql///7q37+//vzzT1WpUkXjx4/Xl19+aW3z999/68aNG9YjBwD8H87pBoCHTEREhDw8PDRq1Kg0z7NMuQJuyh+Yd+4FTbkK8u1S7tt7Z7j28PBQoUKFrOeTpsjMvYVbt24te3t7RUZGpqrFMAyb25c9aJ9++qnNNpwxY4Zu3rxpDWZNmjSRo6OjJk+ebFP7559/rosXL+rJJ5/MUD+urq6ptq10a4zu3CbffvvtXc+pvptq1aopKChIEydOTNVfSj++vr4KDw/XJ598ohMnTqRax72uWO/g4KAaNWpo+/btma6vRo0a8vX11ccff2y9tZJ069ZG+/bts25Pf39/VaxYUXPmzFF8fLy13dq1axUTE3PXPpKSklKdOuHr6yt/f3+bPl1dXTN0ioWPj48aNWqkL774QseOHbOZl9WjJ7y8vPTyyy/r559/tt4+qW3bttq8ebN+/vnnVO0vXLigmzdvSrr1/k9MTNRnn31mnZ+cnGy9XV5GtG3bVklJSXr//fdTzbt586b1tXP+/PlUz7FKlSqSZN2Wd75/7ezsrFe1vn173y6jr4P7lT9/fr311lvat2+f3nrrrTTH68svv9TWrVslSS1atNDWrVu1efNm6/wrV67o008/VWBgoPW+2m3bttXff/9tMwYprl69qitXrmRL/Vl97WX0d0RmpHzBc+fnSkJCgq5du2YzrVSpUnJ3d081/jt27JAk1atXL9P9A7kde7oB4CHj4eGhGTNm6KWXXlK1atX0wgsvyMfHR8eOHdPSpUtVv359TZ06VR4eHtbbaSUmJqpo0aJasWJFmnsoU27lMmTIEL3wwgtycHBQy5Yt5erqqu7du2vMmDHq3r27atSooXXr1ungwYMZrrdUqVL673//q7fffltHjhxRq1at5O7urri4OC1evFg9e/bUgAEDsm37ZMaNGzf0+OOPq23btjpw4ICmT5+uBg0a6D//+Y+kW3/0vv3224qMjNQTTzyh//znP9Z2NWvW1IsvvpihfqpXr64ZM2bov//9r0qXLi1fX1899thjeuqpp/Tee++pS5cuqlevnmJiYjRv3rwsnXss3Qo8M2bMUMuWLVWlShV16dJFRYoU0f79+7Vnzx5roJs2bZoaNGig0NBQ9ejRQyVLltS///6rzZs36/jx46nuE36np59+WkOGDNGlS5dSnR+amJio//73v6mW8fb21muvvaaxY8eqS5cuCgsLU7t27ay3igoMDNSbb75pbT9q1Cg9/fTTql+/vrp06aLz589r6tSpqlixok0Qv9Ply5dVrFgxtWnTRpUrV5abm5t++eUXbdu2TePHj7e2q169ur755hv169dPNWvWlJubm1q2bJnmOidPnqwGDRqoWrVq6tmzp4KCgnTkyBEtXbo0y/cc7tOnjyZOnKgxY8Zo/vz5GjhwoH744Qc99dRT6ty5s6pXr64rV64oJiZGCxcu1JEjR1SoUCG1atVKtWrVUv/+/XXo0CGFhITohx9+0Llz5yRlbA9+WFiYXn75ZY0ePVq///67mjVrJgcHB/3555/69ttvNWnSJLVp00azZ8/W9OnT9cwzz6hUqVK6fPmyPvvsM3l4eFiPBunevbvOnTunxx57TMWKFdPRo0c1ZcoUValSJd09mg4ODhl+HdyvgQMHas+ePRo/frzWrFmjNm3ayM/PTydPntSSJUu0detWbdq0SZI0ePBgff3112revLneeOMNeXt7a/bs2YqLi9N3331nPUz/pZde0oIFC/TKK69ozZo1ql+/vpKSkrR//34tWLBAP//8c5rX48iKrLz2Mvo7IjNcXFxUvnx5ffPNNypbtqy8vb1VsWJF3bx50/oZWr58eeXLl0+LFy/Wv//+qxdeeMFmHStXrlTx4sW5XRiQlgd+vXQAyOPSukVWWtasWWNEREQYnp6ehrOzs1GqVCmjc+fOxvbt261tjh8/bjzzzDOGl5eX4enpaTz33HPGP//8k+atX95//32jaNGihp2dnc3tZxISEoxu3boZnp6ehru7u9G2bVvj1KlT6d4y7PTp02nW+9133xkNGjQwXF1dDVdXVyMkJMTo1auXceDAgUxvj/RuTZXR2/OkrHPt2rVGz549jQIFChhubm5Ghw4djLNnz6ZafurUqUZISIjh4OBgFC5c2Hj11VdT3ZIrvb4N49atep588knD3d3dkGS9fdi1a9eM/v37G0WKFDFcXFyM+vXrG5s3bzbCwsJsbjGWcsul228xZRjp39Jtw4YNRtOmTQ13d3fD1dXVqFSpUqpbfMXGxhodO3Y0/Pz8DAcHB6No0aLGU089ZSxcuDDN53C7f//918iXL58xd+5cm+kptx9K61+pUqWs7b755hujatWqhpOTk+Ht7W106NDBOH78eKp+5s+fb4SEhBhOTk5GxYoVjR9++MF49tlnjZCQEJt2t78Wr1+/bgwcONCoXLmy9flXrlzZmD59us0y8fHxRvv27Q0vLy+b25Clt013795tfS85OzsbwcHBxtChQ++6nVLWld7tlDp37mzY29sbhw4dMgzj1q2e3n77baN06dKGo6OjUahQIaNevXrGhx9+aHNru9OnTxvt27c33N3dDU9PT6Nz587Gxo0bDUk2t8u71y3cPv30U6N69eqGi4uL4e7uboSGhhqDBg0y/vnnH8MwDOO3334z2rVrZxQvXtxwcnIyfH19jaeeesrmM2bhwoVGs2bNDF9fX8PR0dEoXry48fLLLxsnTpywtrnzlmEpMvI6SO85pHzeZFRKnd7e3ka+fPmMIkWKGM8//7wRHR1t0y42NtZo06aNdZxr1apl/Pjjj6nWd+PGDWPs2LFGhQoVDCcnJ6NAgQJG9erVjcjISOPixYvWdvd7yzDDuPdr785bhqXIyO+IzGzfTZs2GdWrVzccHR2t77kzZ84YvXr1MkJCQgxXV1fD09PTqF27trFgwQKbZZOSkowiRYoY7777bqq+ABiGxTAewJVnAAB4gKKiotSlSxdt27Yt2/ZI5TXdunXTwYMHtX79+gfab5UqVeTj45OpW1TlBUuWLNEzzzyjDRs2qH79+jldDmBjyZIlat++vWJjY1WkSJGcLgd46HBONwAASGX48OHatm2bNm7caMr6ExMTrecxp4iOjtauXbsUHh5uSp+PiqtXr9o8TkpK0pQpU+Th4aFq1arlUFVA+saOHavevXsTuIF0cE43AABIpXjx4qkuoJSd/v77bzVp0kQvvvii/P39tX//fn388cfy8/PTK6+8Ylq/j4LXX39dV69eVd26dXX9+nUtWrRImzZt0qhRozJ92zrgQbj94nQAUiN0AwCAB65AgQKqXr26Zs6cqdOnT8vV1VVPPvmkxowZY71XeF712GOPafz48frxxx917do1lS5dWlOmTFHv3r1zujQAQBZwTjcAAAAAACbhnG4AAAAAAExC6AYAAAAAwCSc040clZycrH/++Ufu7u6yWCw5XQ4AAAAAZIhhGLp8+bL8/f1lZ5f+/mxCN3LUP//8o4CAgJwuAwAAAACy5K+//lKxYsXSnU/oRo5yd3eXdOuF6uHhkcPVwAyJiYlasWKFmjVrJgcHh5wuByZhnPMGxjlvYJxzP8Y4b2CczXfp0iUFBARYM016CN3IUSmHlHt4eBC6c6nExETlz59fHh4efODnYoxz3sA45w2Mc+7HGOcNjPODc6/TZLmQGgAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGCSfDldAAAg95jy7Wkl86sl17LTTZVyZZxzO8Y592OM84bcOM4D2vvmdAlZwp5uAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkeTp0R0dHy2Kx6MKFC+m2sVgsWrJkyQOr6W5GjBihKlWqZGnZl156SaNGjcregu7wwgsvaPz48ab2AQAAAACPklwRuqOiouTl5ZXTZWSr7Az7u3bt0rJly/TGG29ky/rS8+6772rkyJG6ePGiqf0AAAAAwKMiV4Ru3N2UKVP03HPPyc3NzdR+KlasqFKlSunLL780tR8AAAAAeFTkeOgODw9X79691bt3b3l6eqpQoUIaOnSoDMOwtrl+/boGDBigokWLytXVVbVr11Z0dLSkW4eId+nSRRcvXpTFYpHFYtGIESMkSXPnzlWNGjXk7u4uPz8/tW/fXqdOnbqvev/66y+1bdtWXl5e8vb21tNPP60jR45Y53fu3FmtWrXShx9+qCJFiqhgwYLq1auXEhMTrW1OnDihJ598Ui4uLgoKCtJXX32lwMBATZw4UZIUGBgoSXrmmWdksVisj1PMnTtXgYGB8vT01AsvvKDLly+nW29SUpIWLlyoli1b2ky/fv263nrrLQUEBMjJyUmlS5fW559/Lun/Drv/+eefVbVqVbm4uOixxx7TqVOn9NNPP6lcuXLy8PBQ+/btlZCQYLPeli1bav78+ZncqgAAAACQO+XL6QIkafbs2erWrZu2bt2q7du3q2fPnipevLh69OghSerdu7f27t2r+fPny9/fX4sXL9YTTzyhmJgY1atXTxMnTtSwYcN04MABSbLu0U1MTNT777+v4OBgnTp1Sv369VPnzp21bNmyLNWZmJioiIgI1a1bV+vXr1e+fPn03//+V0888YT++OMPOTo6SpLWrFmjIkWKaM2aNTp06JCef/55ValSxfp8OnbsqDNnzig6OloODg7q16+fzZcB27Ztk6+vr2bNmqUnnnhC9vb21nmxsbFasmSJfvzxR50/f15t27bVmDFjNHLkyDRr/uOPP3Tx4kXVqFHDZnrHjh21efNmTZ48WZUrV1ZcXJzOnDlj02bEiBGaOnWq8ufPr7Zt26pt27ZycnLSV199pfj4eD3zzDOaMmWK3nrrLesytWrV0siRI3X9+nU5OTmlquf69eu6fv269fGlS5es2/b2LyaQe6SMK+Obu6WMr0VJOf9tLkxjUZL1f8Y592Kccz/GOG/IjeP8sP09mdF6HorQHRAQoAkTJshisSg4OFgxMTGaMGGCevTooWPHjmnWrFk6duyY/P39JUkDBgzQ8uXLNWvWLI0aNUqenp6yWCzy8/OzWW/Xrl2tP5csWVKTJ09WzZo1FR8fn6VDrb/55hslJydr5syZslgskqRZs2bJy8tL0dHRatasmSSpQIECmjp1quzt7RUSEqInn3xSq1atUo8ePbR//3798ssv2rZtmzUIz5w5U2XKlLH24+PjI0ny8vJK9ZySk5MVFRUld3d3SbcukLZq1ap0Q/fRo0dlb28vX19f67SDBw9qwYIFWrlypZo0aWLdPnf673//q/r160uSunXrprfffluxsbHWtm3atNGaNWtsQre/v79u3LihkydPqkSJEqnWOXr0aEVGRqaavmLFCuXPnz/N54DcYeXKlTldAh6Akq6/5XQJeAAY57yBcc79GOO8ITeNcxb3nZrmzqN+0/NQhO46depYQ6wk1a1bV+PHj1dSUpJiYmKUlJSksmXL2ixz/fp1FSxY8K7r3bFjh0aMGKFdu3bp/PnzSk5OliQdO3ZM5cuXz3Sdu3bt0qFDh6yBN8W1a9cUGxtrfVyhQgWbvdNFihRRTEyMJOnAgQPKly+fqlWrZp1funRpFShQIEM1BAYG2vRfpEiRux4yf/XqVTk5Odls399//1329vYKCwu7a1+VKlWy/ly4cGHlz5/fJpwXLlxYW7dutVnGxcVFUvovwLffflv9+vWzPr506ZICAgLUrFkzeXh43LUePJoSExO1cuVKNW3aVA4ODjldDkySMs6Hr1STIft7L4BHkkVJKun6G+OcyzHOuR9jnDfkxnF+/TmfnC7BRspRu/fyUITuu4mPj5e9vb127NhhE2Ql3XVv9ZUrVxQREaGIiAjNmzdPPj4+OnbsmCIiInTjxo0s11K9enXNmzcv1byUvdOSUgULi8ViDfz3K7PrLlSokBISEnTjxg3r4e8pwTgzfVkslgz1fe7cOUm22+N2Tk5OaR527uDgQCDL5RjjvMGQvZIf/l8tyKKUwxMZ59yNcc79GOO8ITeO88P2t2RG63kotv6WLVtsHv/6668qU6aM7O3tVbVqVSUlJenUqVNq2LBhmss7OjoqKSnJZtr+/ft19uxZjRkzRgEBAZKk7du331ed1apV0zfffCNfX98s75UNDg7WzZs3tXPnTlWvXl2SdOjQIZ0/f96mnYODQ6rnlBUp9/Xeu3ev9efQ0FAlJydr7dq11sPLs8vu3btVrFgxFSpUKFvXCwAAAACPoofinPpjx46pX79+OnDggL7++mtNmTJFffr0kSSVLVtWHTp0UMeOHbVo0SLFxcVp69atGj16tJYuXSrp1iHX8fHxWrVqlc6cOaOEhAQVL15cjo6OmjJlig4fPqwffvhB77///n3V2aFDBxUqVEhPP/201q9fr7i4OEVHR+uNN97Q8ePHM7SOkJAQNWnSRD179tTWrVu1c+dO9ezZUy4uLjaHgAcGBmrVqlU6efJkqkCeGT4+PqpWrZo2bNhgs+5OnTqpa9euWrJkifV5LFiwIMv9pFi/fr313HYAAAAAyOseitDdsWNHXb16VbVq1VKvXr3Up08f9ezZ0zp/1qxZ6tixo/r376/g4GC1atVK27ZtU/HixSVJ9erV0yuvvKLnn39ePj4+GjdunHx8fBQVFaVvv/1W5cuX15gxY/Thhx/eV5358+fXunXrVLx4cbVu3VrlypVTt27ddO3atUzt+Z4zZ44KFy6sRo0a6ZlnnlGPHj3k7u4uZ2dna5vx48dr5cqVCggIUNWqVe+r7u7du6c6JH7GjBlq06aNXnvtNYWEhKhHjx66cuXKffVz7do1LVmyxHqVdgAAAADI6yzG7TfEzgHh4eGqUqWK9R7VedHx48cVEBCgX375RY8//ni2r//q1asKDg7WN998o7p162b7+lPMmDFDixcv1ooVKzK8zKVLl+Tp6amLFy9yIbVcKjExUcuWLVOLFi0euvNwkH1Sxjn2Ss1cc94YUrPTTZVy3cY453KMc+7HGOcNuXGcB7T3vXejByijWSZ3bP1HzOrVqxUfH6/Q0FCdOHFCgwYNUmBgoBo1amRKfy4uLpozZ06q+3BnNwcHB02ZMsXUPgAAAADgUULozgGJiYl65513dPjwYbm7u6tevXqaN2+eqXsBw8PDTVt3iu7du5veBwAAAAA8SnI8dEdHR+d0CQ9cyq3MAAAAAAC520NxITUAAAAAAHIjQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgknw5XQAAIPd4/TkfOTg45HQZMEliYqKWLWOcczvGOfdjjPMGxvnhwZ5uAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJPlyugAAQO6xbf8F2dnzqyW3Sk66KYlxfpjVKV8gp0sAANyBPd0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3WmIjo6WxWLRhQsXsmV9nTt3VqtWre7aJjw8XH379r1rm6ioKHl5eWWphqFDh6pnz55ZWjajBg8erNdff93UPgAAAADgUZKrQ/f9hNTsNGnSJEVFRWVqmcDAQE2cODFb+j958qQmTZqkIUOGZMv60jNgwADNnj1bhw8fNrUfAAAAAHhU5OrQ/bDw9PTM0fA/c+ZM1atXTyVKlDC1n0KFCikiIkIzZswwtR8AAAAAeFQ8tKE7PDxcvXv3Vu/eveXp6alChQpp6NChMgzD2ub69esaMGCAihYtKldXV9WuXVvR0dGSbh0i3qVLF128eFEWi0UWi0UjRoyQJM2dO1c1atSQu7u7/Pz81L59e506dSrDtQ0YMEBPPfWU9fHEiRNlsVi0fPly67TSpUtr5syZklIfXn7lyhV17NhRbm5uKlKkiMaPH5/quR89elRvvvmmtfbb/fzzzypXrpzc3Nz0xBNP6MSJE3etd/78+WrZsqXNtOTkZI0bN06lS5eWk5OTihcvrpEjR0qSjhw5IovFogULFqhhw4ZycXFRzZo1dfDgQW3btk01atSQm5ubmjdvrtOnT9ust2XLlpo/f/49tiAAAAAA5A35crqAu5k9e7a6deumrVu3avv27erZs6eKFy+uHj16SJJ69+6tvXv3av78+fL399fixYv1xBNPKCYmRvXq1dPEiRM1bNgwHThwQJLk5uYmSUpMTNT777+v4OBgnTp1Sv369VPnzp21bNmyDNUVFhammTNnKikpSfb29lq7dq0KFSqk6OhoPfHEE/r7778VGxur8PDwNJcfOHCg1q5dq++//16+vr5655139Ntvv6lKlSqSpEWLFqly5crq2bOn9bmmSEhI0Icffqi5c+fKzs5OL774ogYMGKB58+al2de5c+e0d+9e1ahRw2b622+/rc8++0wTJkxQgwYNdOLECe3fv9+mzfDhwzVx4kQVL15cXbt2Vfv27eXu7q5JkyYpf/78atu2rYYNG2azZ7tWrVo6fvy4jhw5osDAwFT1XL9+XdevX7c+vnTpkqRbY5KYmJjmc8CjLWVcGd/cLWV8k5Nv5nAlMFPK+DLOD6/s+Kzlczv3Y4zzBsbZfBndtg916A4ICNCECRNksVgUHBysmJgYTZgwQT169NCxY8c0a9YsHTt2TP7+/pJu7YFevny5Zs2apVGjRsnT01MWi0V+fn426+3atav155IlS2ry5MmqWbOm4uPjrcH8bho2bKjLly9r586dql69utatW6eBAwdqyZIlkm7tZS9atKhKly6datn4+Hh9/vnn+vLLL/X4449LuvXlQrFixaxtvL29ZW9vb90Tf7vExER9/PHHKlWqlKRbXzy899576dZ67NgxGYZh3UaSdPnyZU2aNElTp05Vp06dJEmlSpVSgwYNbJYdMGCAIiIiJEl9+vRRu3bttGrVKtWvX1+S1K1bt1Tnqqf0c/To0TRD9+jRoxUZGZlq+ooVK5Q/f/50nwcefStXrszpEvAAnD2yNadLwAPAOD+8lmXjZVX43M79GOO8gXE2T0JCQobaPdShu06dOjaHVtetW1fjx49XUlKSYmJilJSUpLJly9osc/36dRUsWPCu692xY4dGjBihXbt26fz580pOTpZ0K6CWL1/+nnV5eXmpcuXKio6OlqOjoxwdHdWzZ08NHz5c8fHxWrt2rcLCwtJcNjY2Vjdu3FDt2rWt07y9vRUcHHzPfiUpf/781sAtSUWKFLnrofFXr16VJDk7O1un7du3T9evX7eG/vRUqlTJ+nPhwoUlSaGhoTbT7uzbxcVFUvovwLffflv9+vWzPr506ZICAgLUrFkzeXh43LUePJoSExO1cuVKNW3aVA4ODjldDkySMs4FA2vJzu6h/tWC+5CcfFNnj2xlnB9iNUO87nsdfG7nfoxx3sA4my/lqN17eWR/Y8bHx8ve3l47duyQvb29zby77a2+cuWKIiIiFBERoXnz5snHx0fHjh1TRESEbty4keH+w8PDFR0dLScnJ4WFhcnb21vlypXThg0btHbtWvXv3z/Lz+1u7nzDWCwWm/Pc71SoUCFJ0vnz5+Xj4yPp/4JxZvpK+fLjzmkpX1ikOHfunCRZ+7qTk5OTnJyc0uyLD4PcjTHOG+zs8snO/pH91YIMYpwfXtn5Ocvndu7HGOcNjLN5MrpdH9oLqUnSli1bbB7/+uuvKlOmjOzt7VW1alUlJSXp1KlTKl26tM2/lEOyHR0dlZSUZLOO/fv36+zZsxozZowaNmyokJCQTF1ELUVYWJg2bNigVatWWc/dDg8P19dff62DBw+mez53qVKl5ODgYPPczp8/r4MHD9q0S6v2rChVqpQ8PDy0d+9e67QyZcrIxcVFq1atuu/132n37t1ycHBQhQoVsn3dAAAAAPCoeahD97Fjx9SvXz8dOHBAX3/9taZMmaI+ffpIksqWLasOHTqoY8eOWrRokeLi4rR161aNHj1aS5culXTrXtfx8fFatWqVzpw5o4SEBBUvXlyOjo6aMmWKDh8+rB9++EHvv/9+pmtr1KiRLl++rB9//NEmdM+bN09FihRJddh7Cjc3N3Xr1k0DBw7U6tWrtXv3bnXu3Fl2drZDERgYqHXr1unvv//WmTNnMl1fCjs7OzVp0kQbNmywTnN2dtZbb72lQYMGac6cOYqNjdWvv/6qzz//PMv9pFi/fr31iucAAAAAkNc91KG7Y8eOunr1qmrVqqVevXqpT58+6tmzp3X+rFmz1LFjR/Xv31/BwcFq1aqVtm3bpuLFi0uS6tWrp1deeUXPP/+8fHx8NG7cOPn4+CgqKkrffvutypcvrzFjxujDDz/MdG0FChRQaGiofHx8FBISIulWEE9OTk73fO4UH3zwgRo2bKiWLVuqSZMmatCggapXr27T5r333tORI0dUqlSpdA/Vzqju3btr/vz5NoeCDx06VP3799ewYcNUrlw5Pf/881na43+n+fPnp7riOgAAAADkVRbjbicE56Dw8HBVqVJFEydOzOlSHnmGYah27dp688031a5dO9P6+emnn9S/f3/98ccfypcvY+f6Xbp0SZ6enrp48SIXUsulEhMTtWzZMrVo0YLziXKxlHH2KVmPc31zseSkmzp9eBPj/BCrU77Afa+Dz+3cjzHOGxhn82U0yzzUe7qRPSwWiz799FPdvGnufVWvXLmiWbNmZThwAwAAAEBuRzrKI6pUqaIqVaqY2kebNm1MXT8AAAAAPGoe2tAdHR2d0yUAAAAAAHBfOLwcAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTZDl0z507V/Xr15e/v7+OHj0qSZo4caK+//77bCsOAAAAAIBHWZZC94wZM9SvXz+1aNFCFy5cUFJSkiTJy8tLEydOzM76AAAAAAB4ZGUpdE+ZMkWfffaZhgwZInt7e+v0GjVqKCYmJtuKAwAAAADgUZal0B0XF6eqVaummu7k5KQrV67cd1EAAAAAAOQGWQrdQUFB+v3331NNX758ucqVK3e/NQEAAAAAkCvky8pC/fr1U69evXTt2jUZhqGtW7fq66+/1ujRozVz5szsrhEAAAAAgEdSlkJ39+7d5eLionfffVcJCQlq3769/P39NWnSJL3wwgvZXSMAAAAAAI+kTIfumzdv6quvvlJERIQ6dOighIQExcfHy9fX14z6AAAAAAB4ZGX6nO58+fLplVde0bVr1yRJ+fPnJ3ADAAAAAJCGLF1IrVatWtq5c2d21wIAAAAAQK6SpXO6X3vtNfXv31/Hjx9X9erV5erqajO/UqVK2VIcAODRUjPESw4ODjldBkySmJioZYcZZwAAMiNLoTvlYmlvvPGGdZrFYpFhGLJYLEpKSsqe6gAAAAAAeIRlKXTHxcVldx0AAAAAAOQ6WQrdJUqUyO46AAAAAADIdbIUuufMmXPX+R07dsxSMQAAAAAA5CZZCt19+vSxeZyYmKiEhAQ5Ojoqf/78hG4AAAAAAJTFW4adP3/e5l98fLwOHDigBg0a6Ouvv87uGgEAAAAAeCRlKXSnpUyZMhozZkyqveAAAAAAAORV2Ra6JSlfvnz6559/snOVAAAAAAA8srJ0TvcPP/xg89gwDJ04cUJTp05V/fr1s6UwAAAAAAAedVkK3a1atbJ5bLFY5OPjo8cee0zjx4/PjroAAAAAAHjkZSl0JycnZ3cdAAAAAADkOlk6p/u9995TQkJCqulXr17Ve++9d99FAQAAAACQG2QpdEdGRio+Pj7V9ISEBEVGRt53UQAAAAAA5AZZCt2GYchisaSavmvXLnl7e993UQAAAAAA5AaZOqe7QIECslgsslgsKlu2rE3wTkpKUnx8vF555ZVsLxIAAAAAgEdRpkL3xIkTZRiGunbtqsjISHl6elrnOTo6KjAwUHXr1s32IgEAAAAAeBRlKnR36tRJkhQUFKR69erJwcHBlKIAAAAAAMgNsnTLsLCwMOvP165d040bN2zme3h43F9VAAAAAADkAlm6kFpCQoJ69+4tX19fubq6qkCBAjb/AAAAAABAFkP3wIEDtXr1as2YMUNOTk6aOXOmIiMj5e/vrzlz5mR3jQAAAAAAPJKydHj5//73P82ZM0fh4eHq0qWLGjZsqNKlS6tEiRKaN2+eOnTokN11AgAAAADwyMnSnu5z586pZMmSkm6dv33u3DlJUoMGDbRu3brsqw4AAAAAgEdYlkJ3yZIlFRcXJ0kKCQnRggULJN3aA+7l5ZVtxQEAAAAA8CjLUuju0qWLdu3aJUkaPHiwpk2bJmdnZ7355psaOHBgthYIAAAAAMCjKkvndL/55pvWn5s0aaL9+/drx44dKl26tCpVqpRtxQE55fRva3K6hFzjZrIhSTqza73y2VlyuBqYJWWcAQAAYCtLoft2165dU4kSJVSiRInsqAcAAAAAgFwjS4eXJyUl6f3331fRokXl5uamw4cPS5KGDh2qzz//PFsLBAAAAADgUZWl0D1y5EhFRUVp3LhxcnR0tE6vWLGiZs6cmW3FAQAAAADwKMtS6J4zZ44+/fRTdejQQfb29tbplStX1v79+7OtOAAAAAAAHmVZCt1///23SpcunWp6cnKyEhMT77soAAAAAABygyyF7vLly2v9+vWppi9cuFBVq1a976IAAAAAAMgNsnT18mHDhqlTp076+++/lZycrEWLFunAgQOaM2eOfvzxx+yuEQAAAACAR1Km9nQfPnxYhmHo6aef1v/+9z/98ssvcnV11bBhw7Rv3z7973//U9OmTc2qFQAAAACAR0qm9nSXKVNGJ06ckK+vrxo2bChvb2/FxMSocOHCZtUHAAAAAMAjK1N7ug3DsHn8008/6cqVK9laEAAAAAAAuUWWLqSW4s4QDgAAAAAA/k+mQrfFYpHFYkk1DQAAAAAApJapc7oNw1Dnzp3l5OQkSbp27ZpeeeUVubq62rRbtGhR9lUIAAAAAMAjKlOhu1OnTjaPX3zxxWwtBgAAAACA3CRToXvWrFlm1QEAAAAAQK5zXxdSAwAAAAAA6SN0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdKehc+fOatWqVbatz2KxaMmSJenOP3LkiCwWi37//fe7ric8PFx9+/bNdP83btxQ6dKltWnTpkwvm5k+AgMDtX37dtP6AAAAAIBHDaH7AThx4oSaN2+e4fbR0dGyWCy6cOFCtvT/8ccfKygoSPXq1cuW9aXF0dFRAwYM0FtvvWVaHwAAAADwqCF0PwB+fn5ycnLKkb4Nw9DUqVPVrVs30/vq0KGDNmzYoD179pjeFwAAAAA8Ch660L1w4UKFhobKxcVFBQsWVJMmTXTlyhXr/JkzZ6pcuXJydnZWSEiIpk+fbp2Xcpj2/PnzVa9ePTk7O6tixYpau3attU1SUpK6deumoKAgubi4KDg4WJMmTcpwfYZhyMfHRwsXLrROq1KliooUKWJ9vGHDBjk5OSkhIUFS6sPLt27dqqpVq8rZ2Vk1atTQzp07bZ5D48aNJUkFChSQxWJR586drfOTk5M1aNAgeXt7y8/PTyNGjLhrvTt27FBsbKyefPJJm+nHjx9Xu3bt5O3tLVdXV9WoUUNbtmyRJI0YMUJVqlTRF198oeLFi8vNzU2vvfaakpKSNG7cOPn5+cnX11cjR460WWeBAgVUv359zZ8//94bEgAAAADygHw5XcDtTpw4oXbt2mncuHF65plndPnyZa1fv16GYUiS5s2bp2HDhmnq1KmqWrWqdu7cqR49esjV1VWdOnWyrmfgwIGaOHGiypcvr48++kgtW7ZUXFycChYsqOTkZBUrVkzffvutChYsqE2bNqlnz54qUqSI2rZte88aLRaLGjVqpOjoaLVp00bnz5/Xvn375OLiov379yskJERr165VzZo1lT9//lTLx8fH66mnnlLTpk315ZdfKi4uTn369LHODwgI0Hfffadnn31WBw4ckIeHh1xcXKzzZ8+erX79+mnLli3avHmzOnfurPr166tp06Zp1rt+/XqVLVtW7u7uNjWEhYWpaNGi+uGHH+Tn56fffvtNycnJ1jaxsbH66aeftHz5csXGxqpNmzY6fPiwypYtq7Vr12rTpk3q2rWrmjRpotq1a1uXq1WrltavX3/P7QgAAAAAecFDF7pv3ryp1q1bq0SJEpKk0NBQ6/zhw4dr/Pjxat26tSQpKChIe/fu1SeffGITunv37q1nn31WkjRjxgwtX75cn3/+uQYNGiQHBwdFRkZa2wYFBWnz5s1asGBBhkK3dOuCZp988okkad26dapatar8/PwUHR2tkJAQRUdHKywsLM1lv/rqKyUnJ+vzzz+Xs7OzKlSooOPHj+vVV1+VJNnb28vb21uS5OvrKy8vL5vlK1WqpOHDh0uSypQpo6lTp2rVqlXphu6jR4/K398/VQ2nT5/Wtm3brH2VLl3apk1ycrK++OILubu7q3z58mrcuLEOHDigZcuWyc7OTsHBwRo7dqzWrFljE7r9/f119OjRdLfd9evXdf36devjS5cuSZISExOVmJiY7nIP2s1kI6dLyDWS/v+2TGKb5mop4/swvY+R/VLGl3HO3Rjn3I8xzhsYZ/NldNs+VKG7cuXKevzxxxUaGqqIiAg1a9ZMbdq0UYECBXTlyhXFxsaqW7du6tGjh3WZmzdvytPT02Y9devWtf6cL18+1ahRQ/v27bNOmzZtmr744gsdO3ZMV69e1Y0bN1SlSpUM1xkWFqY+ffro9OnTWrt2rcLDw62hu1u3btq0aZMGDRqU5rL79u1TpUqV5OzsnGa991KpUiWbx0WKFNGpU6fSbX/16lWbviTp999/V9WqVa2BOy2BgYE2e8cLFy4se3t72dnZ2Uy7s28XFxfrYfVpGT16tM2XHilWrFiR5pEByD12nIjP6RLwAKxcuTKnS8ADwDjnDYxz7scY5w2Ms3nulntu91CFbnt7e61cuVKbNm3SihUrNGXKFA0ZMkRbtmyxBrLPPvvMZs9qynIZNX/+fA0YMEDjx49X3bp15e7urg8++MB6PnNGhIaGytvbW2vXrtXatWs1cuRI+fn5aezYsdq2bZsSExNNu1K4g4ODzWOLxWJzWPidChUqpJiYGJtptx+unpl+MtL3uXPn5OPjk+563377bfXr18/6+NKlSwoICFCzZs3k4eFxz7oelDO7OEQ+uyQlG9pxIl7Vi7jJ3s6S0+XAJCnj3LRp01SfFcg9EhMTtXLlSsY5l2Occz/GOG9gnM2XctTuvTxUoVu6FeTq16+v+vXra9iwYSpRooQWL16sfv36yd/fX4cPH1aHDh3uuo5ff/1VjRo1knRrT/iOHTvUu3dvSdLGjRtVr149vfbaa9b2sbGxma6xYcOG+v7777Vnzx41aNBA+fPn1/Xr1/XJJ5+oRo0acnV1TXPZcuXKae7cubp27Zp1D/Svv/5q08bR0VHSrYu+3a+qVatqxowZMgxDFsutwFOpUiXNnDlT586du+ve7qzYvXu3qlatmu58JyenNK/k7uDg8FB9GOQjHGY7ezsL2zUPeNjeyzAH45w3MM65H2OcNzDO5snodn2orl6+ZcsWjRo1Stu3b9exY8e0aNEinT59WuXKlZMkRUZGavTo0Zo8ebIOHjyomJgYzZo1Sx999JHNeqZNm6bFixdr//796tWrl86fP6+uXbtKunUe9Pbt2/Xzzz/r4MGDGjp0qLZt25bpWsPDw/X111+rSpUqcnNzk52dnRo1aqR58+alez63JLVv314Wi0U9evTQ3r17tWzZMn344Yc2bUqUKCGLxaIff/xRp0+fVnx81g/Lbdy4seLj421u49WuXTv5+fmpVatW2rhxow4fPqzvvvtOmzdvznI/KdavX69mzZrd93oAAAAAIDd4qEK3h4eH1q1bpxYtWqhs2bJ69913NX78eDVv3lyS1L17d82cOVOzZs1SaGiowsLCFBUVpaCgIJv1jBkzRmPGjFHlypW1YcMG/fDDDypUqJAk6eWXX1br1q31/PPPq3bt2jp79qzNXu+MCgsLU1JSksLDw63TwsPDU027k5ubm/73v/8pJiZGVatW1ZAhQzR27FibNkWLFlVkZKQGDx6swoULW/fSZ0XBggX1zDPPaN68edZpjo6OWrFihXx9fdWiRQuFhoZqzJgxmTpMPy2bN2/WxYsX1aZNm/taDwAAAADkFhYj5X5cucCRI0cUFBSknTt3ZurCaLndH3/8oaZNmyo2NlZubm6m9fP888+rcuXKeueddzK8zKVLl+Tp6amLFy8+VOd0n/5tTU6XkGvcTDa09e/LqlXUncPLc7GUcW7RogWHsOViiYmJWrZsGeOcyzHOuR9jnDcwzubLaJZ5qPZ0wxyVKlXS2LFjFRcXZ1ofN27cUGhoqN58803T+gAAAACAR81DdyE1mKNz586mrt/R0VHvvvuuqX0AAAAAwKMmV4XuwMBA5aKj5QEAAAAAjzgOLwcAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAk+XK6AOBh5FOtcU6XkGskJiZKfy9TocoN5eDgkNPlwCQp4wwAAABb7OkGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMki+nCwAA5B5HpkXK3kjK6TKQRSXfHJ3TJQAAkOuwpxsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMEmeD92dO3dWq1at0p0fFRUlLy+vB1bPvQQGBmrixImZXu7s2bPy9fXVkSNHsr2mFGfOnJGvr6+OHz9uWh8AAAAA8CjJ86H7YZXdYX/kyJF6+umnFRgYmG3rvFOhQoXUsWNHDR8+3LQ+AAAAAOBRQujOAxISEvT555+rW7dupvfVpUsXzZs3T+fOnTO9LwAAAAB42OVo6F64cKFCQ0Pl4uKiggULqkmTJrpy5Yp1/syZM1WuXDk5OzsrJCRE06dPt847cuSILBaL5s+fr3r16snZ2VkVK1bU2rVrrW2SkpLUrVs3BQUFycXFRcHBwZo0adJ91/3999+rWrVqcnZ2VsmSJRUZGambN29a51ssFs2cOVPPPPOM8ufPrzJlyuiHH36wWccPP/ygMmXKyNnZWY0bN9bs2bNlsVh04cIFRUdHq0uXLrp48aIsFossFotGjBhhXTYhIUFdu3aVu7u7ihcvrk8//fSu9S5btkxOTk6qU6eOzfQ9e/boqaeekoeHh9zd3dWwYUPFxsZK+r/D7keNGqXChQvLy8tL7733nm7evKmBAwfK29tbxYoV06xZs2zWWaFCBfn7+2vx4sVZ2bQAAAAAkKvkWOg+ceKE2rVrp65du2rfvn2Kjo5W69atZRiGJGnevHkaNmyYRo4cqX379mnUqFEaOnSoZs+ebbOegQMHqn///tq5c6fq1q2rli1b6uzZs5Kk5ORkFStWTN9++6327t2rYcOG6Z133tGCBQuyXPf69evVsWNH9enTR3v37tUnn3yiqKgojRw50qZdZGSk2rZtqz/++EMtWrRQhw4drHt/4+Li1KZNG7Vq1Uq7du3Syy+/rCFDhliXrVevniZOnCgPDw+dOHFCJ06c0IABA6zzx48frxo1amjnzp167bXX9Oqrr+rAgQN3rbl69eo20/7++281atRITk5OWr16tXbs2KGuXbvafHmwevVq/fPPP1q3bp0++ugjDR8+XE899ZQKFCigLVu26JVXXtHLL7+c6hzuWrVqaf369ZnfuAAAAACQy+TLqY5PnDihmzdvqnXr1ipRooQkKTQ01Dp/+PDhGj9+vFq3bi1JCgoKsobcTp06Wdv17t1bzz77rCRpxowZWr58uT7//HMNGjRIDg4OioyMtLYNCgrS5s2btWDBArVt2zZLdUdGRmrw4MHWGkqWLKn3339fgwYNsjmXuXPnzmrXrp0kadSoUZo8ebK2bt2qJ554Qp988omCg4P1wQcfSJKCg4O1e/dua3B3dHSUp6enLBaL/Pz8UtXQokULvfbaa5Kkt956SxMmTNCaNWsUHBycZs1Hjx6Vv7+/zbRp06bJ09NT8+fPl4ODgySpbNmyNm28vb01efJk2dnZKTg4WOPGjVNCQoLeeecdSdLbb7+tMWPGaMOGDXrhhResy/n7+2vnzp1p1nL9+nVdv37d+vjSpUuSpMTERCUmJqa5DB5tKePK+OZuKeObZOGspUfZvd6nvJ/zBsY592OM8wbG2XwZ3bY5FrorV66sxx9/XKGhoYqIiFCzZs3Upk0bFShQQFeuXFFsbKy6deumHj16WJe5efOmPD09bdZTt25d68/58uVTjRo1tG/fPuu0adOm6YsvvtCxY8d09epV3bhxQ1WqVMly3bt27dLGjRtt9mwnJSXp2rVrSkhIUP78+SVJlSpVss53dXWVh4eHTp06JUk6cOCAatasabPeWrVqZbiG29edEsxT1p2Wq1evytnZ2Wba77//roYNG1oDd1oqVKggO7v/+wO6cOHCqlixovWxvb29ChYsmKpvFxcXJSQkpLnO0aNH23wRkmLFihXWbYfcaeXKlTldAh6AA4HV790ID629y5ZlqB3v57yBcc79GOO8gXE2T3qZ5045Frrt7e21cuVKbdq0SStWrNCUKVM0ZMgQbdmyxRq+PvvsM9WuXTvVchk1f/58DRgwQOPHj1fdunXl7u6uDz74QFu2bMly3fHx8YqMjLTugb/d7cH2zjBrsViUnJyc5X5vl9l1FypUSOfPn7eZ5uLikqV+MtL3uXPn5OPjk+Y63377bfXr18/6+NKlSwoICFCzZs3k4eFxz5rw6ElMTNTKlSvVtGnTu37Jg0dbyjgHH9kheyN7Puvw4AX2uvvdJ3g/5w2Mc+7HGOcNjLP5Uo7avZccC93SrcBWv3591a9fX8OGDVOJEiW0ePFi9evXT/7+/jp8+LA6dOhw13X8+uuvatSokaRbe8J37Nih3r17S5I2btyoevXqWQ/FlmS9UFhWVatWTQcOHFDp0qWzvI7g4GAtu2NvwrZt22weOzo6KikpKct93K5q1ar68ssvbaZVqlRJs2fPVmJiYra/CXfv3q3w8PA05zk5OcnJySnVdAcHBz4McjnGOG+wN5Jlb2TPZxcevIy+R3k/5w2Mc+7HGOcNjLN5Mrpdc+zkuy1btmjUqFHavn27jh07pkWLFun06dMqV66cpFvnTo8ePVqTJ0/WwYMHFRMTo1mzZumjjz6yWc+0adO0ePFi7d+/X7169dL58+fVtWtXSVKZMmW0fft2/fzzzzp48KCGDh2aKtxm1rBhwzRnzhxFRkZqz5492rdvn+bPn6933303w+t4+eWXtX//fr311ls6ePCgFixYoKioKEm3voiQpMDAQMXHx2vVqlU6c+ZMhg9dSEtERIT27Nljs7e7d+/eunTpkl544QVt375df/75p+bOnXvXC7JlREJCgnbs2KFmzZrd13oAAAAAIDfIsdDt4eGhdevWqUWLFipbtqzeffddjR8/Xs2bN5ckde/eXTNnztSsWbMUGhqqsLAwRUVFKSgoyGY9Y8aM0ZgxY1S5cmVt2LBBP/zwgwoVKiTpVrht3bq1nn/+edWuXVtnz5612eudFREREfrxxx+1YsUK1axZU3Xq1NGECROsF4PLiKCgIC1cuFCLFi1SpUqVNGPGDOvVy1P2AterV0+vvPKKnn/+efn4+GjcuHFZrjk0NFTVqlWzuWp7wYIFtXr1asXHxyssLEzVq1fXZ599dt/fgn3//fcqXry4GjZseF/rAQAAAIDcwGKk3KPrEXPkyBEFBQVp586d93VhtIfFyJEj9fHHH+uvv/4yZf1Lly7VwIEDtXv3bpuLo2W3OnXq6I033lD79u0z1P7SpUvy9PTUxYsXOac7l0pMTNSyZcvUokULDm3KxVLGuXzcNg4vf4SVfHP0Xefzfs4bGOfcjzHOGxhn82U0y+ToOd152fTp01WzZk0VLFhQGzdu1AcffGA9F90MTz75pP7880/9/fffCggIMKWPM2fOqHXr1tZbpQEAAABAXkfoziF//vmn/vvf/+rcuXMqXry4+vfvr7ffftvUPvv27Wvq+gsVKqRBgwaZ2gcAAAAAPEoe2dAdGBioR/TIeEnShAkTNGHChJwuAwAAAABgohy7kBoAAAAAALkdoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwSb6cLgAAkHsE9houBweHnC4DAADgocGebgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAEySL6cLQN5mGIYk6dKlSzlcCcySmJiohIQEXbp0SQ4ODjldDkzCOOcNjHPewDjnfoxx3sA4my8lw6RkmvQQupGjLl++LEkKCAjI4UoAAAAAIPMuX74sT0/PdOdbjHvFcsBEycnJ+ueff+Tu7i6LxZLT5cAEly5dUkBAgP766y95eHjkdDkwCeOcNzDOeQPjnPsxxnkD42w+wzB0+fJl+fv7y84u/TO32dONHGVnZ6dixYrldBl4ADw8PPjAzwMY57yBcc4bGOfcjzHOGxhnc91tD3cKLqQGAAAAAIBJCN0AAAAAAJiE0A3AVE5OTho+fLicnJxyuhSYiHHOGxjnvIFxzv0Y47yBcX54cCE1AAAAAABMwp5uAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AaQ7c6dO6cOHTrIw8NDXl5e6tatm+Lj4++6THh4uCwWi82/V1555QFVjIyYNm2aAgMD5ezsrNq1a2vr1q13bf/tt98qJCREzs7OCg0N1bJlyx5QpbgfmRnnqKioVO9bZ2fnB1gtMmvdunVq2bKl/P39ZbFYtGTJknsuEx0drWrVqsnJyUmlS5dWVFSU6XXi/mR2nKOjo1O9ly0Wi06ePPlgCkamjR49WjVr1pS7u7t8fX3VqlUrHThw4J7L8bs5ZxC6AWS7Dh06aM+ePVq5cqV+/PFHrVu3Tj179rzncj169NCJEyes/8aNG/cAqkVGfPPNN+rXr5+GDx+u3377TZUrV1ZERIROnTqVZvtNmzapXbt26tatm3bu3KlWrVqpVatW2r179wOuHJmR2XGWJA8PD5v37dGjRx9gxcisK1euqHLlypo2bVqG2sfFxenJJ59U48aN9fvvv6tv377q3r27fv75Z5Mrxf3I7DinOHDggM372dfX16QKcb/Wrl2rXr166ddff9XKlSuVmJioZs2a6cqVK+kuw+/mnMPVywFkq3379ql8+fLatm2batSoIUlavny5WrRooePHj8vf3z/N5cLDw1WlShVNnDjxAVaLjKpdu7Zq1qypqVOnSpKSk5MVEBCg119/XYMHD07V/vnnn9eVK1f0448/WqfVqVNHVapU0ccff/zA6kbmZHaco6Ki1LdvX124cOEBV4rsYLFYtHjxYrVq1SrdNm+99ZaWLl1q80f5Cy+8oAsXLmj58uUPoErcr4yMc3R0tBo3bqzz58/Ly8vrgdWG7HP69Gn5+vpq7dq1atSoUZpt+N2cc9jTDSBbbd68WV5eXtbALUlNmjSRnZ2dtmzZctdl582bp0KFCqlixYp6++23lZCQYHa5yIAbN25ox44datKkiXWanZ2dmjRpos2bN6e5zObNm23aS1JERES67ZHzsjLOkhQfH68SJUooICBATz/9tPbs2fMgysUDwns5b6lSpYqKFCmipk2bauPGjTldDjLh4sWLkiRvb+902/B+zjn5croAALnLyZMnUx2Oli9fPnl7e9/13LD27durRIkS8vf31x9//KG33npLBw4c0KJFi8wuGfdw5swZJSUlqXDhwjbTCxcurP3796e5zMmTJ9Nsz/mBD6+sjHNwcLC++OILVapUSRcvXtSHH36oevXqac+ePSpWrNiDKBsmS++9fOnSJV29elUuLi45VBmyU5EiRfTxxx+rRo0aun79umbOnKnw8HBt2bJF1apVy+nycA/Jycnq27ev6tevr4oVK6bbjt/NOYfQDSBDBg8erLFjx961zb59+7K8/tvP+Q4NDVWRIkX0+OOPKzY2VqVKlcryegGYp27duqpbt671cb169VSuXDl98sknev/993OwMgCZERwcrODgYOvjevXqKTY2VhMmTNDcuXNzsDJkRK9evbR7925t2LAhp0tBOgjdADKkf//+6ty5813blCxZUn5+fqkuunTz5k2dO3dOfn5+Ge6vdu3akqRDhw4RunNYoUKFZG9vr3///ddm+r///pvumPr5+WWqPXJeVsb5Tg4ODqpataoOHTpkRonIAem9lz08PNjLncvVqlWLEPcI6N27t/Witfc6wojfzTmHc7oBZIiPj49CQkLu+s/R0VF169bVhQsXtGPHDuuyq1evVnJysjVIZ8Tvv/8u6dYhb8hZjo6Oql69ulatWmWdlpycrFWrVtns5bxd3bp1bdpL0sqVK9Ntj5yXlXG+U1JSkmJiYnjf5iK8l/Ou33//nffyQ8wwDPXu3VuLFy/W6tWrFRQUdM9leD/nIAMAstkTTzxhVK1a1diyZYuxYcMGo0yZMka7du2s848fP24EBwcbW7ZsMQzDMA4dOmS89957xvbt2424uDjj+++/N0qWLGk0atQop54C7jB//nzDycnJiIqKMvbu3Wv07NnT8PLyMk6ePGkYhmG89NJLxuDBg63tN27caOTLl8/48MMPjX379hnDhw83HBwcjJiYmJx6CsiAzI5zZGSk8fPPPxuxsbHGjh07jBdeeMFwdnY29uzZk1NPAfdw+fJlY+fOncbOnTsNScZHH31k7Ny50zh69KhhGIYxePBg46WXXrK2P3z4sJE/f35j4MCBxr59+4xp06YZ9vb2xvLly3PqKSADMjvOEyZMMJYsWWL8+eefRkxMjNGnTx/Dzs7O+OWXX3LqKeAeXn31VcPT09OIjo42Tpw4Yf2XkJBgbcPv5ocHoRtAtjt79qzRrl07w83NzfDw8DC6dOliXL582To/Li7OkGSsWbPGMAzDOHbsmNGoUSPD29vbcHJyMkqXLm0MHDjQuHjxYg49A6RlypQpRvHixQ1HR0ejVq1axq+//mqdFxYWZnTq1Mmm/YIFC4yyZcsajo6ORoUKFYylS5c+4IqRFZkZ5759+1rbFi5c2GjRooXx22+/5UDVyKg1a9YYklL9SxnXTp06GWFhYamWqVKliuHo6GiULFnSmDVr1gOvG5mT2XEeO3asUapUKcPZ2dnw9vY2wsPDjdWrV+dM8ciQtMZXks37k9/NDw/u0w0AAAAAgEk4pxsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAmO7kyZNq2rSpXF1d5eXlle40i8WiJUuWZGidI0aMUJUqVUyp90F41OsHAGQMoRsAgDzs5MmTev3111WyZEk5OTkpICBALVu21KpVq7K1nwkTJujEiRP6/fffdfDgwXSnnThxQs2bN8/QOgcMGJDtdUZFRVm/AEjP+PHjVaBAAV27di3VvISEBHl4eGjy5MnZWhcA4NFF6AYAII86cuSIqlevrtWrV+uDDz5QTEyMli9frsaNG6tXr17Z2ldsbKyqV6+uMmXKyNfXN91pfn5+cnJyytA63dzcVLBgwWytMyNeeuklXblyRYsWLUo1b+HChbpx44ZefPHFB14XAODhROgGACCPeu2112SxWLR161Y9++yzKlu2rCpUqKB+/frp119/tbY7duyYnn76abm5ucnDw0Nt27bVv//+a7Ou77//XtWqVZOzs7NKliypyMhI3bx5U5IUGBio7777TnPmzJHFYlHnzp3TnCalPrz8+PHjateunby9veXq6qoaNWpoy5YtktI+PHvmzJkqV66cnJ2dFRISounTp1vnHTlyRBaLRYsWLVLjxo2VP39+Va5cWZs3b5YkRUdHq0uXLrp48aIsFossFotGjBiRarv5+vqqZcuW+uKLL1LN++KLL9SqVSt5e3vrrbfeUtmyZZU/f36VLFlSQ4cOVWJiYrrjER4err59+9pMa9WqlXXbSNL169c1YMAAFS1aVK6urqpdu7aio6PTXScAIOfly+kCAADAg3fu3DktX75cI0eOlKura6r5KYdYJycnWwP32rVrdfPmTfXq1UvPP/+8NeytX79eHTt21OTJk9WwYUPFxsaqZ8+ekqThw4dr27Zt6tixozw8PDRp0iS5uLjoxo0bqabdKT4+XmFhYSpatKh++OEH+fn56bffflNycnKaz2nevHkaNmyYpk6dqqpVq2rnzp3q0aOHXF1d1alTJ2u7IUOG6MMPP1SZMmU0ZMgQtWvXTocOHVK9evU0ceJEDRs2TAcOHJB0a296Wrp166annnpKR48eVYkSJSRJhw8f1rp16/Tzzz9Lktzd3RUVFSV/f3/FxMSoR48ecnd316BBgzIwQmnr3bu39u7dq/nz58vf31+LFy/WE088oZiYGJUpUybL6wUAmIfQDQBAHnTo0CEZhqGQkJC7tlu1apViYmIUFxengIAASdKcOXNUoUIFbdu2TTVr1lRkZKQGDx5sDbYlS5bU+++/r0GDBmn48OHy8fGRk5OTXFxc5OfnZ113WtNu99VXX+n06dPatm2bvL29JUmlS5dOt9bhw4dr/Pjxat26tSQpKChIe/fu1SeffGITugcMGKAnn3xSkhQZGakKFSro0KFDCgkJkaenpywWS7o1pYiIiJC/v79mzZpl3RseFRWlgIAAPf7445Kkd99919o+MDBQAwYM0Pz587Mcuo8dO6ZZs2bp2LFj8vf3tz6X5cuXa9asWRo1alSW1gsAMBehGwCAPMgwjAy127dvnwICAqyBW5LKly8vLy8v7du3TzVr1tSuXbu0ceNGjRw50tomKSlJ165dU0JCgvLnz5+lGn///XdVrVrVGrjv5sqVK4qNjVW3bt3Uo0cP6/SbN2/K09PTpm2lSpWsPxcpUkSSdOrUqXt+AXE7e3t7derUSVFRURo+fLgMw9Ds2bPVpUsX2dndOnvvm2++0eTJkxUbG6v4+HjdvHlTHh4eGe7jTjExMUpKSlLZsmVtpl+/fj1Hzm0HAGQMoRsAgDyoTJkyslgs2r9//32vKz4+XpGRkdY9zLdzdnbO8nrTOuT8bjVI0meffabatWvbzLO3t7d57ODgYP3ZYrFIUrqHrN9N165dNXr0aK1evVrJycn666+/1KVLF0nS5s2b1aFDB0VGRioiIkKenp6aP3++xo8fn+767OzsUn0Zcvs54PHx8bK3t9eOHTtSPaf0DoMHAOQ8QjcAAHmQt7e3IiIiNG3aNL3xxhupzuu+cOGCvLy8VK5cOf3111/666+/rHu79+7dqwsXLqh8+fKSpGrVqunAgQN3PfQ7KypVqqSZM2fq3Llz99zbXbhwYfn7++vw4cPq0KFDlvt0dHRUUlJShtqWKlVKYWFh+uKLL2QYhpo0aWI9v3vTpk0qUaKEhgwZYm1/9OjRu67Px8dHJ06csD5OSkrS7t271bhxY0lS1apVlZSUpFOnTqlhw4aZfWoAgBzC1csBAMijpk2bpqSkJNWqVUvfffed/vzzT+3bt0+TJ09W3bp1JUlNmjRRaGioOnTooN9++01bt25Vx44dFRYWpho1akiShg0bpjlz5igyMlJ79uzRvn37NH/+fJtzmrOiXbt28vPzU6tWrbRx40YdPnxY3333nfVq43eKjIzU6NGjNXnyZB08eFAxMTGaNWuWPvroowz3GRgYqPj4eK1atUpnzpxRQkLCXdt369ZNixYt0uLFi9WtWzfr9DJlyujYsWOaP3++YmNjNXnyZC1evPiu63rssce0dOlSLV26VPv379err76qCxcuWOeXLVtWHTp0UMeOHbVo0SLFxcVp69atGj16tJYuXZrh5wgAeLAI3QAA5FElS5bUb7/9psaNG6t///6qWLGimjZtqlWrVmnGjBmSbh1+/f3336tAgQJq1KiRmjRpopIlS+qbb76xriciIkI//vijVqxYoZo1a6pOnTqaMGGCda9vVjk6OmrFihXy9fVVixYtFBoaqjFjxqQ6tDpF9+7dNXPmTM2aNUuhoaEKCwtTVFSUgoKCMtxnvXr19Morr+j555+Xj4+Pxo0bd9f2zz77rJycnJQ/f361atXKOv0///mP3nzzTfXu3VtVqlTRpk2bNHTo0Luuq2vXrurUqZP1S42SJUta93KnmDVrljp27Kj+/fsrODhYrVq10rZt21S8ePEMP0cAwINlMTJ6JRUAAAAAAJAp7OkGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABM8v8AmduygVYvV0wAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":["'''\n","19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n","Score'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import cohen_kappa_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","\n","# Evaluate using Cohen's Kappa Score\n","kappa = cohen_kappa_score(y_test, y_pred)\n","print(f\"Cohen's Kappa Score: {kappa:.2f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGXkMFipmcUV","executionInfo":{"status":"ok","timestamp":1750863669917,"user_tz":-330,"elapsed":23,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"7fbcddbd-65e0-4841-cb06-1f85af8bc54f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Cohen's Kappa Score: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","20.  Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n","classification\n","'''\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_curve, average_precision_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Logistic Regression\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Predict probabilities\n","y_scores = model.predict_proba(X_test)[:, 1]\n","\n","# Compute Precision-Recall values\n","precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n","average_precision = average_precision_score(y_test, y_scores)\n","\n","# Plot the Precision-Recall Curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, label=f'AP = {average_precision:.2f}')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve - Logistic Regression')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"vdbjE2Tym2h3","executionInfo":{"status":"ok","timestamp":1750863763558,"user_tz":-330,"elapsed":579,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"b94b65e7-8a5f-41c7-cbf3-e4e1bd9671dd"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["'''\n","21.  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n","their accuracy\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with actual values\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# List of solvers to compare\n","solvers = ['liblinear', 'saga', 'lbfgs']\n","\n","# Dictionary to store accuracy for each solver\n","solver_accuracies = {}\n","\n","# Train and evaluate model with each solver\n","for solver in solvers:\n","    try:\n","        model = LogisticRegression(solver=solver, max_iter=1000)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        accuracy = accuracy_score(y_test, y_pred)\n","        solver_accuracies[solver] = accuracy\n","        print(f\"Solver: {solver}, Accuracy: {accuracy:.2f}\")\n","    except Exception as e:\n","        print(f\"Solver: {solver}, Error: {str(e)}\")\n","\n","# Summary\n","print(\"\\nAccuracy Comparison by Solver:\")\n","for solver, acc in solver_accuracies.items():\n","    print(f\"{solver}: {acc:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EExPDNuEnNPL","executionInfo":{"status":"ok","timestamp":1750863838488,"user_tz":-330,"elapsed":108,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"cf57b3ad-e6e6-47fe-bf04-363ddbb17a92"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Solver: liblinear, Accuracy: 1.00\n","Solver: saga, Accuracy: 1.00\n","Solver: lbfgs, Accuracy: 1.00\n","\n","Accuracy Comparison by Solver:\n","liblinear: 1.00\n","saga: 1.00\n","lbfgs: 1.00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["'''\n","22.  Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n","Correlation Coefficient (MCC)\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with actual values\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# List of solvers to compare\n","solvers = ['liblinear', 'saga', 'lbfgs']\n","\n","# Dictionary to store accuracy for each solver\n","solver_accuracies = {}\n","\n","# Train and evaluate model with each solver\n","for solver in solvers:\n","    try:\n","        model = LogisticRegression(solver=solver, max_iter=1000)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        accuracy = accuracy_score(y_test, y_pred)\n","        solver_accuracies[solver] = accuracy\n","        print(f\"Solver: {solver}, Accuracy: {accuracy:.2f}\")\n","    except Exception as e:\n","        print(f\"Solver: {solver}, Error: {str(e)}\")\n","\n","# Summary\n","print(\"\\nAccuracy Comparison by Solver:\")\n","for solver, acc in solver_accuracies.items():\n","    print(f\"{solver}: {acc:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPIK3WzrnfrK","executionInfo":{"status":"ok","timestamp":1750863920867,"user_tz":-330,"elapsed":99,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"a71fcb52-a5bf-4e43-ac9f-be1486f81b16"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Solver: liblinear, Accuracy: 1.00\n","Solver: saga, Accuracy: 1.00\n","Solver: lbfgs, Accuracy: 1.00\n","\n","Accuracy Comparison by Solver:\n","liblinear: 1.00\n","saga: 1.00\n","lbfgs: 1.00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["'''\n","23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n","accuracy to see the impact of feature scaling\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Logistic Regression on raw data\n","model_raw = LogisticRegression(max_iter=1000)\n","model_raw.fit(X_train, y_train)\n","y_pred_raw = model_raw.predict(X_test)\n","accuracy_raw = accuracy_score(y_test, y_pred_raw)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Logistic Regression on standardized data\n","model_scaled = LogisticRegression(max_iter=1000)\n","model_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = model_scaled.predict(X_test_scaled)\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","# Print accuracy comparison\n","print(f\"Accuracy on raw data: {accuracy_raw:.2f}\")\n","print(f\"Accuracy on standardized data: {accuracy_scaled:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_9X-PH6nzxz","executionInfo":{"status":"ok","timestamp":1750864003745,"user_tz":-330,"elapsed":71,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"963cf3f4-a47b-4207-b316-9784c1a8f8da"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on raw data: 1.00\n","Accuracy on standardized data: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","24.  Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n","cross-validation\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define Logistic Regression model\n","model = LogisticRegression(max_iter=1000)\n","\n","# Define range of C values for tuning\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n","\n","# Apply GridSearchCV with 5-fold cross-validation\n","grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n","grid_search.fit(X_train, y_train)\n","\n","# Output best parameters and evaluate on test set\n","best_c = grid_search.best_params_['C']\n","print(f\"Best C value from cross-validation: {best_c}\")\n","\n","# Predict and calculate accuracy\n","y_pred = grid_search.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Test Accuracy with best C: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfsT23eYoIBf","executionInfo":{"status":"ok","timestamp":1750864123448,"user_tz":-330,"elapsed":1341,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"64f93b98-2144-4975-acc9-01a6de885683"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Best C value from cross-validation: 0.01\n","Test Accuracy with best C: 1.00\n"]}]},{"cell_type":"code","source":["'''\n","25.  Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n","make predictions.\n","'''\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import joblib\n","\n","# Load dataset\n","# Replace 'data.csv' and 'target' with your actual file and target column\n","data = pd.read_csv('iris.csv')\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Ensure binary classification\n","unique_classes = y.unique()\n","if len(unique_classes) > 2:\n","    data = data[data['target'].isin(unique_classes[:2])]\n","    X = data.drop('target', axis=1)\n","    y = data['target']\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train model\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Save model\n","joblib.dump(model, 'logistic_model.pkl')\n","print(\"Model saved as 'logistic_model.pkl'\")\n","\n","# Load model\n","loaded_model = joblib.load('logistic_model.pkl')\n","\n","# Make predictions\n","predictions = loaded_model.predict(X_test)\n","accuracy = accuracy_score(y_test, predictions)\n","print(f\"Accuracy of loaded model: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oUpBszkUok7f","executionInfo":{"status":"ok","timestamp":1750864210096,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ashif Sarkar","userId":"15004214289662800785"}},"outputId":"f116fbc2-9263-47ad-9791-847c32da9b9f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved as 'logistic_model.pkl'\n","Accuracy of loaded model: 1.00\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kedsEL19o6Zx"},"execution_count":null,"outputs":[]}]}